[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/basis-analysis-synthesis/index.html",
    "href": "posts/basis-analysis-synthesis/index.html",
    "title": "Drop the BAS",
    "section": "",
    "text": "There always seems to be a lot of startups building things like note-taking, dating and educational apps. These are seductive because the people building them are likely to be young, single and recently out of college, and it’s much easier to build something that solves a problem you know well. It’s harder to build something in a space that you don’t have day-to-day experience.\nPeople who work on solutions in unusual niches like payments processing can hit on really lucrative solutions to pain points by leveraging their experience. So how do you gain that experience?"
  },
  {
    "objectID": "posts/basis-analysis-synthesis/index.html#the-trivium",
    "href": "posts/basis-analysis-synthesis/index.html#the-trivium",
    "title": "Drop the BAS",
    "section": "The Trivium",
    "text": "The Trivium\nBack in the middle ages, after hundreds of years of sifting through the materials from ancient Greece and Rome, a method of teaching the “liberal arts” emerged that followed the pattern:\n\nGrammar\nLogic\nRhetoric\n\nGrammar focused on the underlying mechanics of language, such as vocabulary and sentence structure. It also focused on rote learning and memorization. It’s probably similar to how primary education works (or at least how it worked when I was a child), where you learn how to read and write, and also learn things like common sayings and simple poems.\nLogic was a development on grammar. Going beyond the direct meaning of words, it dealt with how the ideas they expressed held together. The purpose was to question the material, understand the arguments being made, and generally think critically about the material you were reading.\nFinally, rhetoric took the ideas that you learned in critiquing others, and taught you to create new ideas yourself, and to argue their validity in a persuasive way.\nIn a way, these could be thought of as:\n\nGrammar = what?\nLogic = why?/how?\nRhetoric = what next?"
  },
  {
    "objectID": "posts/basis-analysis-synthesis/index.html#bas",
    "href": "posts/basis-analysis-synthesis/index.html#bas",
    "title": "Drop the BAS",
    "section": "BAS",
    "text": "BAS\nAt the time I read this, I was becoming a senior consulting civil engineer in Canada. That meant that I was the one teaching junior engineers how to approach their work. As always happens, you really deepen your understanding of a subject when you go to teach it to someone else. Anyway, I liked the general progression of the trivium, and I could see that it didn’t just apply to a liberal education.\nShooting from the hip doesn’t really work in civil engineering. It’s expensive to create drawings, and redoing them over and over is a great way to waste time. Instead, you need to be methodical and start at the beginning. Each project is basically a set of inputs and constraints, with a target that you are trying to achieve. The process I came up with to do this methodically was:\n\nBasis (derived from the greek for “foundation”)\nAnalysis (“breaking up”)\nSynthesis (“composition”)\n\nThe basis of a project is the bare facts, so these should be assembled first:\n\nWho is the client? What are they trying to achieve?\nWhere is the site? What are the conditions there like?\nWhat are the constraints? e.g. legal, environmental, cost.\n\nWith all the facts, analysis is then an interrogation of what you have assembled. The more fully you have gathered the facts, the more straightforward the analysis usually becomes. The outcome of the analysis is a mental model for how the elements of the project fit together. You can infer underlying mechanisms from the observations you have made, and from that assess how the clients goals conflict with conditions on the ground.\nFinally, once you’ve made a model for how the project fits together, you can then start to experiment and create the design for something new.\nThere was a great side effect to this way of working: because everything flowed logically together in design, it made it easy to write clear, logical reports that explained why a project had to be done a certain way. Good communication makes it easy to retain clients."
  },
  {
    "objectID": "posts/basis-analysis-synthesis/index.html#beyond-engineering",
    "href": "posts/basis-analysis-synthesis/index.html#beyond-engineering",
    "title": "Drop the BAS",
    "section": "Beyond Engineering",
    "text": "Beyond Engineering\nCivil engineering wasn’t the end of the story. I left many years ago, and I’ve since worked in roles that range between operations and product management (with a strong technical weighting) in the finance and insurance industries.\nThe same procedure still works. In fact it’s probably more important. Often, I’m approached by people who come with solutions rather than problems, which is at the wrong end of the process. Starting by assembling facts, then analyzing them leads to much more effective solutions than merely taking a solution spec and working through it.\nIt might seem here that I’m advocating for a waterfall-style product delivery method, but that’s not the case. In fact, this process supplements lean innovation methods very neatly.\n\nLean Innovation\nLean innovation is an iterative process that relies on developing assumptions, running measurable experiments to test those assumptions, then improving (learning). A typical product might go through the following stages 1:\n\nProblem - Solution fit\nProduct - Market fit\nScaling\n\nPeople like to say that “there’s no bad ideas”, but from experience I can say that there are definitely good ones. I’ve done a lot of experimentation in my career, and I can tell you that you can have better ideas in areas where you know how things work than when you do not.\nWith the goal of moving more wisely, I try to learn as much as I can before starting to develop assumptions. This puts lean innovation into the synthesis stage of the process, so we have:\n\nBasis - gather facts: interview users in the field, review consultant reports, examine existing solutions to see who they are targeting and get a sense of their views of the problems.\nAnalysis - assemble the facts together: try to fit these into a mental model to try and infer underlying mechanisms that are giving rise to your observations.\nSynthesis - build: armed with a model for how your problem space fits together, you can now make assumptions for how your interventions will play out. Better assumptions are more likely to lead to better experiment results."
  },
  {
    "objectID": "posts/basis-analysis-synthesis/index.html#summary",
    "href": "posts/basis-analysis-synthesis/index.html#summary",
    "title": "Drop the BAS",
    "section": "Summary",
    "text": "Summary\nKnowing more is better than knowing less. Lean methods are tactically correct, but strategically it can be hard to find your way if you don’t know the terrain. Methodically following a process of assembling a basis of facts then analyzing them to assemble a model of the underlying mechanism for your problem space can set the stage for a much more effective synthesis of something new."
  },
  {
    "objectID": "posts/basis-analysis-synthesis/index.html#footnotes",
    "href": "posts/basis-analysis-synthesis/index.html#footnotes",
    "title": "Drop the BAS",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGriesbach, D. 2023. Lean Innovation Guide↩︎"
  },
  {
    "objectID": "posts/hidden-markov-model/index.html",
    "href": "posts/hidden-markov-model/index.html",
    "title": "Finding Markov",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport seaborn as sns\nfrom IPython.display import HTML, Markdown, display\nfrom matplotlib.ticker import PercentFormatter, ScalarFormatter\nfrom tabulate import tabulate\n\npalette = sns.color_palette()\nSometimes one model of the world won’t do. Often, we try to fit a well-known model to some observations we have made, and just doesn’t fit. Here we will look at the particular situation where observations are generated by several distributions using a mechanism known as a Hidden Markov Model (HMM).\nBelow, we will look at what a Hidden Markov Model is, with a basic example. Then we will create a model to simulate the S&P500. The model does a fairly good job of picking out the major panics of the past 150 years. Interestingly, it indicates that the chance of slipping into a panic in any given month over the period was about 1%. Finally, we will use the model to create some simulations, showing that these models have some advantages in being able to incorporate the risk of a severely negative event that Gaussian models by themselves cannot."
  },
  {
    "objectID": "posts/hidden-markov-model/index.html#hidden-markov-models",
    "href": "posts/hidden-markov-model/index.html#hidden-markov-models",
    "title": "Finding Markov",
    "section": "Hidden Markov Models",
    "text": "Hidden Markov Models\nThe idea is that the system we are trying to model has several states, and the distribution of observations varies depending on the state that the system is in. You could imagine a call center with a helpful and an unhelpful representative, the results of reaching one will be better than the results of reaching the other.\nHowever, with Hidden Markov Models, it is not quite as easy as working with call center representatives, as we will not get to know who is on the other line. All we get are the results, and we have to try and piece together the details of which representative we got based on the outcomes we saw, and then try to model the distribution of outcomes for each of them.\n\nToy Example: Moody Dog\nSay we have a dog, and that this dog eats more food when it’s happy. Let’s say the dog eats the following quantities (both assumed to be normally distributed):\n\nWhen happy, the dog eats an average of 250g, with a standard deviation of 50g.\nWhen sad, the dog eats an average of 150g, with a standard deviation of 30g.\n\nThe above distributions are known as emission probability distributions, modelling the emitted observation (how much the dog eats), based on the internal state of the dog (its mood).\nWe will model the mood of our dog (i.e. the dog’s state) as a Markov Process. By this, we just mean that the dog’s mood one day is dependent only on it’s mood the previous day. It does not matter if the dog was happy or sad on any of the days before that.\nOur Markov Process has discrete time as we step from day to day, and a discrete and countable number of states, i.e. happy and sad. We will model our dog’s transition from day to day as follows:\n\nHappy today:\n\n70% chance of being happy tomorrow,\n30% chance of being sad.\n\nSad today:\n\n50% chance of being happy tomorrow,\n50% chance of being sad.\n\n\nWe will write this as a matrix like so, where \\(P_{ij}\\) means the probability of going from row \\(i\\) to column \\(j\\), arranged so that row and column 0 represent “sad”, and row and column 1 represent “happy”.\n\\[\nP_{ij} =\n\\begin{bmatrix}\n0.5 & 0.5 \\\\\n0.3 & 0.7\n\\end{bmatrix}\n\\]\nWe will assume that the initial probabilities are the steady state probabilities of the dog being either happy or sad, calculated as follows:\n\n\nCode\ntransition_matrix = np.array([\n    [0.5, 0.5],\n    [0.3, 0.7]\n])\n# Need left eigenvector\np = np.linalg.eig(transition_matrix.T).eigenvectors[:, 1]\n# Normalize so probabilities sum to one.\ninitial_probabilities = p / np.sum(p)\ninitial_probabilities\n\n\narray([0.375, 0.625])\n\n\nThat means that in the long run, the dog is expected to be happy 62.5% of the time, and sad 37.5% of the time.\nKnowing all this we can then simulate a few days and see how much the dog eats:\n\n\nCode\nn_days = 25\nsimulated_data = []\nrng = np.random.default_rng(1)\nhappy = rng.binomial(n=1, p=0.625)\nmu = [150, 250]\nsigma = [30, 50]\nfor day in range(n_days):\n    food_eaten = rng.normal(mu[happy], sigma[happy])\n    simulated_data.append({\n        \"day\": day,\n        \"happy\": happy,\n        \"food_eaten\": food_eaten\n    })\n    happy = rng.binomial(n=1, p=transition_matrix[happy, 1])\n\nsimulated_data = pd.DataFrame(simulated_data).set_index(\"day\")\n\nfig, ax = plt.subplots()\nsns.lineplot(data=simulated_data, x=\"day\", y=\"food_eaten\", ax=ax)\n\ny1, y2 = ax.get_ylim()\nfor i, row in simulated_data.iterrows():\n    if row[\"happy\"]:\n        ax.fill_between(\n            [i-0.5, i + 0.5], \n            y1=y1, \n            y2=y2, \n            color='green',\n            edgecolor=None, \n            alpha=0.2,\n            label=\"happy\"\n            )\n\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.show()\n\n\n\n\n\nFigure 1: Simulated Amounts of Food Eaten by Dog (g), Happy Days Shaded Green\n\n\n\n\nAnd, we can see how the distributions are combined together:\n\n\nCode\nx = np.linspace(50, 400, 1000)\nfig, ax = plt.subplots()\nplt.plot(x, stats.norm.pdf(x, mu[0], sigma[0]), label=\"sad\")\nplt.plot(x, stats.norm.pdf(x, mu[1], sigma[1]), label=\"happy\")\nplt.plot(x, \n    0.375 * stats.norm.pdf(x, mu[0], sigma[0])\n        + 0.625 * stats.norm.pdf(x, mu[1], sigma[1]),\n    label=\"combined\")\n\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(frameon=False)\nax.set_yticks([])\nax.set_yticklabels([])\nplt.xlabel(\"food_eaten\")\nplt.ylabel(\"probability_density\")\nplt.show()\n\n\n\n\n\nFigure 2: Probability Density Functions for Amounts Eaten by Dog (g) Based on Mood\n\n\n\n\nIn the figures above, we can see that there is some overlap between the amounts eaten when happy and when sad. Some days the dog will eat more when sad than it does on another day when happy. As good dog owners, we would know whether our dog is happy or not, but in general, we will not know the internal state of the system we are trying to model.\nThe unseen state is the “hidden” part of a Hidden Markov Model. There are some ways to estimate which state is generating the observations, and once we can estimate this, we can estimate parameters for the system and create simulations."
  },
  {
    "objectID": "posts/hidden-markov-model/index.html#something-more-serious-separating-stock-returns",
    "href": "posts/hidden-markov-model/index.html#something-more-serious-separating-stock-returns",
    "title": "Finding Markov",
    "section": "Something More Serious: Separating Stock Returns",
    "text": "Something More Serious: Separating Stock Returns\nNow we will look at something where we don’t know the underlying probability distributions: stock market returns. The reason these are interesting is that a plot of returns (daily, monthly etc.) looks like a Gaussian distribution, but big losses happen more frequently that would be expected under a Gaussian distribution of returns.\n\nData\nThe S&P 500 index price data was retrieved from Robert Shiller’s Online Data website, and contains a reconstituted time series back to 1871.\n\n\nCode\ndf = pd.read_pickle(\"posts/hidden-markov-model/data/shiller_data.pkl\")\ndata = (df[\"s&p_comp_p\"].pct_change())[1:].to_numpy()\ndf[\"s&p_perf\"] = np.insert(data, 0, np.nan)\n\nsns.lineplot(df[\"s&p_comp_p\"])\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.yscale('log')\nformatter = ScalarFormatter()\nformatter.set_scientific(False)\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.ylabel(\"s&p500_index\")\nplt.show()\n\n\n\n\n\nFigure 3: Price of the S&P 500 Index\n\n\n\n\nThe performance numbers we are analysing are monthly returns in that dataset, which are distributed like this:\n\n\nCode\nsns.histplot(df[\"s&p_perf\"], edgecolor=None)\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.gca().xaxis.set_major_formatter(PercentFormatter(1))\nplt.ylabel(\"count\")\nplt.xlabel(\"s&p500_monthly_performance\")\nplt.show()\n\n\n\n\n\nFigure 4: Distribution of the S&P 500 Index Monthly Performance\n\n\n\n\nThis shows that the distribution has a negative skew, with a visibly fat tail on the left side. The datapoint with a +50% performance would warrant a deeper review of the data if this analysis was being relied on in a meaningful way. It occurred in September 1932 in the dataset, but a google search did not provide a ready explanation.\n\n\nModel Description\nIn the toy example above, we assumed that there were 2 states that the dog could be in, and each was associated with a different emission probability distribution. For stock returns, we will assume that there are 3 states:\n\nNormal - This will be the case the majority of the time, characterized by a Gaussian distribution.\nRelief - This will be a rare case, where large gains are made, characterized by a Pareto distribution. (I originally called this “euphoria”, but as we see below it usually happens during a pull back after a panic).\nPanic - Another rare case, were large losses are made, characterized by a Pareto distribution.\n\nMany people are familiar with the Gaussian Distribution, the classic “bell curve” with a mean and standard deviation. Fewer are aware of the Pareto distribution, though we might have heard of the 80/20 rule that inspired it. I’m using it here as it is suggested by Nassim Taleb in his Technical Incerto. It is a distribution with fat tails, with the following probability density function (pdf):\n\\[\nP(X &gt; x) = \\begin{cases}\n\\frac{\\alpha k^{\\alpha}}{x^{(\\alpha + 1)}} & \\text{for } x \\geq k, \\\\\n0 & \\text{for } x &lt; k.\n\\end{cases}\n\\]\nWhere:\n\n\\(\\alpha &gt; 0\\) is the shape parameter, determining the thickness of the tails, where a lower value means thicker tails.\n\\(k &gt; 0\\) is the scale parameter, setting the minimum value for the distribution.\n\nAn interesting interpretation of the scale parameter is as follows. Say the populations of our cities are Pareto-distributed with \\(\\alpha=2\\). If there is a 10% probability that a city has a population greater than 10 million, then the probability that there are more than twice that number (i.e. more than 20 million) would be a \\(10\\% \\times (1/2)^\\alpha = 10\\% \\times (1/2)^2 = 2.5\\%\\).\n\n\nCode\nalpha = 3 \nk = 1\n\nx = np.linspace(k, 4, 100)\npdf = stats.pareto.pdf(x, alpha, loc=0, scale=k) \n\nfig, ax = plt.subplots()\nplt.plot(x, pdf, label=f'α={alpha}, k={k}')\n\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.legend(frameon=False)\nplt.xlabel(\"x\")\nplt.ylabel(\"probability_density\")\nax.set_yticks([])\nax.set_yticklabels([])\nplt.xlim([1, 4])\nplt.show()\n\n\n\n\n\nFigure 5: Probability Density Function for Generic Pareto Distribution\n\n\n\n\n\n\nModel Fitting\nThe general approach to fitting the model will be Maximum Likelihood Estimation (MLE). However, we are fitting 2 models which interact with each other, not just one. So, to allow ourselves to do that we will use dynamic programming, following a general algorithm like this:\n\nStart with assumed transition probability distribution and emission probability distributions.\nGiven the transition probability, find the optimal parameters of the emission probability distributions to fit the data through MLE.\nGiven the updated emission probability distributions, find the optimal transition probabilities using the forward-backward algorithm.\nRepeat steps 2. and 3. until convergence.\n\nAs this is a numerical method, it can be prone to finding local optima, so in general it would be run several times with different initial assumptions. However, looking at the results, this wasn’t necessary here.\n\n\nCode\nclass HMM:\n    def __init__(self):\n        # Probability of being in each state at t=0\n        self.initial = np.array([0.1, 0.8, 0.1])\n        # Probability of being in a final state, given that you are in that state\n        self.final = np.ones(3)\n        # Initial transition matrix, from column to row.\n        self.transitions = np.array(\n            [\n                [0.1, 0.8, 0.1],\n                [0.1, 0.8, 0.1],\n                [0.1, 0.8, 0.1],\n            ]\n        )\n        # Initial parameters for emission probability distributions\n        self.emissions = [\n            {\n                \"type\": stats.pareto,\n                \"params\": {\n                    \"b\": 1,\n                    \"loc\": 0.0,\n                    \"scale\": 0.1,\n                },\n                \"fit_params\": {\n                    \"floc\": 0.0, # Loc overlaps with the scale parameter, fix at 0\n                },\n                \"negative_emission\": True,\n            },\n            {\n                \"type\": stats.norm,\n                \"params\": {\n                    \"loc\": 0.007,\n                    \"scale\": 0.01,\n                },\n                \"fit_params\": {},\n                \"negative_emission\": False,\n            },\n            {\n                \"type\": stats.pareto,\n                \"params\": {\n                    \"b\": 1,\n                    \"loc\": 0.0,\n                    \"scale\": 0.1,\n                },\n                \"fit_params\": {\n                    \"floc\": 0.0, # Loc overlaps with the scale parameter, fix at 0\n                },\n                \"negative_emission\": False,\n            },\n        ]\n\n    def _pdf_emissions(self, Y: np.array):\n        \"\"\"Return value of pdf at each of the emissions.\"\"\"\n        pdfs = np.zeros(shape=(Y.shape[0], len(self.emissions)))\n        for i, distribution in enumerate(self.emissions):\n            # Required for positive-only Pareto Distribution\n            if distribution[\"negative_emission\"] == True:\n                _Y = -Y\n            else:\n                _Y = Y\n\n            pdfs[:, i] = distribution[\"type\"].pdf(x=_Y, **distribution[\"params\"])\n\n        return pdfs\n\n    def _forward_pass(self, Y: np.array, pdfs: np.array):\n        \"\"\"Return normalized probabilities of emission for each state given history.\"\"\"\n        alpha = np.zeros(shape=(Y.shape[0], len(self.emissions)))\n        alpha[0, :] = self.initial * pdfs[0, :]\n        norm = 1 / np.sum(alpha[0, :])\n        alpha[0, :] = alpha[0, :] * norm\n\n        for i in range(1, Y.shape[0]):\n            alpha[i, :] = (alpha[i - 1, :] @ self.transitions) * pdfs[i, :]\n            norm = 1 / np.sum(alpha[i, :])\n            alpha[i, :] = alpha[i, :] * norm\n        return alpha\n\n    def _backward_pass(self, Y: np.array, pdfs: np.array):\n        \"\"\"Return normalized probabilities of emission for each state given future.\"\"\"\n        beta = np.zeros(shape=(Y.shape[0], len(self.emissions)))\n        beta[-1, :] = self.final\n\n        for i in range(Y.shape[0] - 2, -1, -1):\n            beta[i, :] = (beta[i + 1, :] @ self.transitions.T) * pdfs[i + 1, :]\n            norm = 1 / np.sum(beta[i, :])\n            beta[i, :] = beta[i, :] * norm\n\n        return beta\n\n    def _update_state_transitions(self, Y: np.array):\n        pdfs = self._pdf_emissions(Y)\n        pdfs[pdfs == 0] = 1e-100\n\n        alpha = self._forward_pass(Y, pdfs=pdfs)\n        beta = self._backward_pass(Y, pdfs=pdfs)\n\n        gamma = alpha * beta\n        gamma = gamma / np.sum(gamma, axis=1)[:, None]\n\n        xi = (\n            alpha[:-1, :, None]\n            * self.transitions[None, :, :]\n            * beta[1:, None, :]\n            * pdfs[1:, None, :]\n        )\n        norm = 1 / np.sum(xi, axis=(1, 2))[:, None, None]\n        xi = xi * norm\n\n        self.initial = gamma[0, :]\n        # NOTE: added initial here so sum doesn't become zero.\n        self.transitions = (\n            np.sum(xi, axis=0, initial=1e-20)\n            / np.sum(gamma, axis=0, initial=1e-20)[:, None]\n        )\n        norm = 1 / np.sum(self.transitions, axis=1)[:, None]\n        self.transitions = self.transitions * norm\n        self.gamma = gamma\n        self.states = np.argmax(gamma, axis=1)\n\n    def fit(self, Y: np.array, n_iter=300):\n        for _ in range(n_iter):\n            self._update_state_transitions(Y)\n            for i, distribution in enumerate(self.emissions):\n                _Y = Y[self.states == i]\n                if distribution[\"negative_emission\"] == True:\n                    _Y = -_Y\n\n                if distribution[\"type\"] == stats.pareto:\n                    _Y = _Y[_Y &gt; 0]\n                    if len(_Y) &gt; 0:\n                        params = stats.pareto.fit(\n                            data=_Y,\n                            **distribution[\"fit_params\"],\n                        )\n                        distribution[\"params\"] = {\n                            \"b\": params[0],\n                            \"loc\": params[1],\n                            \"scale\": params[2],\n                        }\n                elif distribution[\"type\"] == stats.norm:\n                    if len(_Y) &gt; 0:\n                        params = stats.norm.fit(\n                            data=_Y,\n                            **distribution[\"fit_params\"],\n                        )\n                        distribution[\"params\"] = {\n                            \"loc\": params[0],\n                            \"scale\": params[1],\n                        }\n\n    def simulate(self, n_periods, burn_in_periods=0, seed=None):\n        rng = np.random.default_rng(seed)\n        np.random.seed(seed) # Used by scipy stats\n\n        all_selections = []\n        all_emissions = []\n        for row in range(self.transitions.shape[0]):\n            all_selections.append(\n                rng.choice(\n                    a=len(self.initial),\n                    size=n_periods + burn_in_periods,\n                    p=self.transitions[row, :]\n                )\n            )\n            e = self.emissions[row][\"type\"].rvs(\n                **self.emissions[row][\"params\"], size=n_periods + burn_in_periods\n            )\n            if self.emissions[row][\"negative_emission\"]:\n                e = -e\n            all_emissions.append(e)\n\n        all_selections = np.vstack(all_selections).T\n        all_emissions = np.vstack(all_emissions).T\n\n        states = []\n        emissions = []\n        for row in range(n_periods + burn_in_periods):\n            if row == 0:\n                s = rng.choice(len(self.initial), p=self.initial)\n            else:\n                s = all_selections[row, s]\n\n            if row &gt;= burn_in_periods:\n                states.append(s)\n                emissions.append(all_emissions[row, s])\n\n        return emissions, states\n\nhmm = HMM()\nhmm.fit(data)\ndf[\"hmm_states\"] = np.insert(hmm.states, 0, 0)\n\n\nFirst, lets see how the model fit the states to the data:\n\n\nCode\nlabels = [\"panic\", \"normal\", \"relief\"]\nfig, ax1 = plt.subplots()\nax1.plot(df.index, df[\"s&p_comp_p\"], color=palette[0], label=\"s&p500\")\nax2 = ax1.twinx()\nax2.plot(df.index, df[\"hmm_states\"], color=\"#b0b0b0\", label=\"state\")\n\nax2.set_yticks([0, 1, 2])\nax2.set_yticklabels(labels)\n\nsns.despine(left=True, bottom=True, top=True, right=True)\nax1.set_yscale('log')\nformatter = ScalarFormatter()\nformatter.set_scientific(False)\nax1.yaxis.set_major_formatter(formatter)\n\nax1.set_zorder(1) # Plot ax1 on top of ax2\nax1.patch.set_visible(False) # Stop ax1 from hiding ax2\n# fig.legend(frameon=False, loc=(0.15,.8))\nax1.set_ylabel(\"s&p500_index\")\nax1.set_xlabel(\"date\")\nplt.show()\n\n\n\n\n\nFigure 6: S&P 500 Index with Estimated States\n\n\n\n\nThis plot does indeed seem to pick up many of the notable market panics from the past hundred years, including the crash that started the great depression in the late 1920s, the tech bubble bursting in the early 2000s, the great financial crisis, etc. Notably, covid was not detected as the market fell and corrected within a month, so the crash isn’t present in the month-end data.\nI called the positive distribution “relief”, as it seems to only occur in periods following a panic. Let’s see the transition probabilities:\n\n\nCode\nfloat_format = [\".1%\" for _ in range(hmm.transitions.shape[1] + 1)] \npretty = tabulate(hmm.transitions, headers=labels, showindex = labels, tablefmt=\"html\", floatfmt=float_format)\ndisplay(HTML(pretty))\n\n\n\n\nTable 1: S&P 500 State Transition Probabilities, showing probability of transitioning from state on vertical to the state on the horizontal.\n\n\n\npanic\nnormal\nrelief\n\n\n\n\npanic\n10.8%\n83.8%\n5.4%\n\n\nnormal\n1.0%\n98.9%\n0.1%\n\n\nrelief\n0.0%\n67.4%\n32.6%\n\n\n\n\n\n\nThis tells us that the default state here is to stay in the “business as usual” normal state. From there, there is a 98.9% chance that the S&P 500 will stay in that state, following a Gaussian Distribution from one month to the next, with a 1.0% chance of moving into a panic state and a 0.1% chance of moving to a euphoric state.\nAs we noticed above, euphoria seems to be the wrong term for periods of unusually positive performance. It is more likely that we reach that state following a state of panic (\\(1.0\\%\\times 5.4\\% = 0.5\\%\\)) than following a normal state (\\(0.1\\%\\)). Perhaps it might be better termed “relief”.\nFinally, let’s look at the parameters for the emission probability distributions:\n\n\nCode\nprint(\nf\"panic:\\n  shape: {hmm.emissions[0]['params']['b']:0.2f}\\n  scale: -{hmm.emissions[0]['params']['scale']:0.2%}\\n\\\nnormal:\\n  mean: {hmm.emissions[1]['params']['loc']:0.2%}\\n  st.dev.: {hmm.emissions[1]['params']['scale']:0.2%}\\n\\\neuphoria:\\n  shape: {hmm.emissions[2]['params']['b']:0.2f}\\n  scale: {hmm.emissions[2]['params']['scale']:0.2%}\"\n)\n\n\npanic:\n  shape: 1.65\n  scale: -5.03%\nnormal:\n  mean: 0.54%\n  st.dev.: 3.57%\neuphoria:\n  shape: 0.87\n  scale: 5.03%\n\n\nAnd let’s plot the emission probability distribution over histograms of returns for periods when the model estimates that the market was in each state:\n\n\nCode\np = np.linalg.eig(hmm.transitions.T).eigenvectors[:, 0]\n# Normalize so probabilities sum to one.\nsteady_state_probabilities = p / np.sum(p)\nsteady_state_probabilities\n\ndef plot_states(state, ymax):\n    fig, ax1 = plt.subplots()\n    n_points = 200\n    xmin = -0.3\n    xmax=0.5\n    x = np.linspace(xmin, xmax, n_points)\n    dist = hmm.emissions[state]\n    y = dist[\"type\"].pdf(x, **dist[\"params\"]) #* steady_state_probabilities[state]\n    if dist[\"negative_emission\"]:\n        x = np.linspace(-xmin, -xmax, n_points)\n\n    ax1.plot(x, y, label = labels[state])\n\n    ax2 = ax1.twinx()\n\n    sns.histplot(df.loc[df[\"hmm_states\"]==state, \"s&p_perf\"], edgecolor=None, ax=ax2)\n    sns.despine(left=True, bottom=True, top=True, right=True)\n    plt.gca().xaxis.set_major_formatter(PercentFormatter(1))\n\n    sns.despine(left=True, bottom=True, top=True, right=True)\n    ax1.set_yticks([])\n    ax1.set_yticklabels([])\n    ax1.set_ylim([-0.1, None])\n\n    ax2.set_yticks([])\n    ax2.set_yticklabels([])\n    ax2.set_ylim([-0.01, ymax])\n\n    ax1.set_ylabel(\"probability_density\")\n    ax2.set_ylabel(\"count\")\n    ax1.set_xlabel(\"s&p500_monthly_performance\")\n    plt.show()\n\n\nplot_states(0, 20)\nplot_states(1, 150)\nplot_states(2, 20)\n\n\n\n\n\nFigure 7: Model Fit to Returns for Panic State\n\n\n\n\n\n\n\nFigure 8: Model Fit to Returns for Normal State\n\n\n\n\n\n\n\nFigure 9: Model Fit to Returns for Euphoria State\n\n\n\n\nThese look like reasonable approximations, but more could probably be done to tighten them up.\n\n\nSimulation\nOf course, the fun doesn’t have to end here. We can use the model to create simulations about how the next 150 years might transpire. Here is a single rollout of 150 years:\n\n\nCode\nn_years = 150\nn_months = n_years * 12\nemissions, states = hmm.simulate(n_months, seed=19)\n\nx = np.linspace(1/12, n_years, n_months)\n\nlabels = [\"panic\", \"normal\", \"relief\"]\nfig, ax1 = plt.subplots()\n\nax1.plot(x, np.cumprod(np.array(emissions) + 1), color=palette[0], label=\"s&p500\")\nax2 = ax1.twinx()\nax2.plot(x, states, color=\"#b0b0b0\", label=\"state\")\n\nax2.set_yticks([0, 1, 2])\nax2.set_yticklabels(labels)\n\nsns.despine(left=True, bottom=True, top=True, right=True)\nax1.set_yscale('log')\nformatter = ScalarFormatter()\nformatter.set_scientific(False)\nax1.yaxis.set_major_formatter(formatter)\n\nax1.set_xticks(np.arange(min(x), max(x)+1, 25))\n\nax1.set_zorder(1) # Plot ax1 on top of ax2\nax1.patch.set_visible(False) # Stop ax1 from hiding ax2\n# fig.legend(frameon=False, loc=(0.15,.8))\nax1.set_ylabel(\"s&p500_index\")\nax1.set_xlabel(\"years\")\nplt.show()\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\nThe above chart looks much like the last 150 years: generally up, with some “lost decades”.\nAn interesting issue is that the model will sometimes generate a month of ruin, which looks like this:\n\n\nCode\nn_years = 150\nn_months = n_years * 12\nemissions, states = hmm.simulate(n_months, seed=18)\n\nx = np.linspace(1/12, n_years, n_months)\n\nlabels = [\"panic\", \"normal\", \"relief\"]\nfig, ax1 = plt.subplots()\n\nax1.plot(x, np.cumprod(np.array(emissions) + 1), color=palette[0], label=\"s&p500\")\nax2 = ax1.twinx()\nax2.plot(x, states, color=\"#b0b0b0\", label=\"state\")\n\nax2.set_yticks([0, 1, 2])\nax2.set_yticklabels(labels)\n\nsns.despine(left=True, bottom=True, top=True, right=True)\nax1.set_yscale('log')\nformatter = ScalarFormatter()\nformatter.set_scientific(False)\nax1.yaxis.set_major_formatter(formatter)\n\nax1.set_xticks(np.arange(min(x), max(x)+1, 25))\n\nax1.set_zorder(1) # Plot ax1 on top of ax2\nax1.patch.set_visible(False) # Stop ax1 from hiding ax2\n# fig.legend(frameon=False, loc=(0.15,.8))\nax1.set_ylabel(\"s&p500_index\")\nax1.set_xlabel(\"years\")\nplt.show()\n\n\n\n?(caption)\n\n\n\n\n\n\n\n\nOn the face of it, it looks like this might be an issue with the model, but actually this should be expected. We are looking at many periods of 150 years where wealth in the market is occasionally wiped out. For a few examples over the period where this actually happened in some of the most developed markets in the world, I found this interesting graphic:"
  },
  {
    "objectID": "posts/hidden-markov-model/index.html#summary",
    "href": "posts/hidden-markov-model/index.html#summary",
    "title": "Finding Markov",
    "section": "Summary",
    "text": "Summary\nAbove, we explained what a Hidden Markov Model (HMM) is with a simple example. Then we plunged into a much more interesting example: we showed that a HMM can be used to explain some of the biggest market incidents of the past 150 years, and can be used to simulate a future in which substantial and real negative tail event can be incorporated in a simulation."
  },
  {
    "objectID": "posts/hidden-markov-model/index.html#further-reading",
    "href": "posts/hidden-markov-model/index.html#further-reading",
    "title": "Finding Markov",
    "section": "Further Reading",
    "text": "Further Reading\nShiller, R. (2023), Online Data - Robert Shiller, Home Page of Robert Shiller, available at http://www.econ.yale.edu/~shiller/data.htm (accessed October 5, 2023).\nTaleb, N. N. (2020), Statistical Consequences of Fat Tails: Real World Preasymptotics, Epistemology, and Applications: Papers and Commentary, The Technical Incerto Collection, USA? STEM Academic Press."
  },
  {
    "objectID": "posts/one-armed-bandit/index.html",
    "href": "posts/one-armed-bandit/index.html",
    "title": "Decision-Making - Beating the Bandits",
    "section": "",
    "text": "Decisions are hard. We can’t see how things will turn out, and we never have all the information we need to make the best decision right now. However, there are systematic ways to work through the choices we make to give us a better chance of coming out on top.\nOne way to view things is through the lens of reinforcement learning. This is a flexible framework for decision making that assumes there is some environment out there to act on, and whatever we decide to do the environment gives us some feedback in the form of a reward or a punishment. Through these interactions, we can learn a better decision making rule.\nThere is a little more to the framework, such as the idea of a “state” that can impact the results of our actions, and there are diferent ways of setting up a model for analysis. But, we will ignore that for now for the classic model I will talk about below: the multi-armed bandit. The way I set this up allows for the following types of decisions:\nYou could apply the above to decisions such as:\nIn all the above, we have some number of options, and we keep gong back to make these decisions repeatedly. Each option can generally get better or worse over time as staff get more experienced or leave, but we generally don’t know what impact that will have until we get to see how it plays out, and each one of them can have days where they uncharacteristically knock it out of the park or just do a terrible job.\nBesides being actually useful, this type of decision is also nice because it’s relatively simple and it demonstrates some really nice elements of a decision-making strategy, which are the main takeaways from this post:"
  },
  {
    "objectID": "posts/one-armed-bandit/index.html#the-multi-armed-bandit-model",
    "href": "posts/one-armed-bandit/index.html#the-multi-armed-bandit-model",
    "title": "Decision-Making - Beating the Bandits",
    "section": "The Multi-Armed Bandit Model",
    "text": "The Multi-Armed Bandit Model\nThe actual model we will look at to demonstrate the idea is called the “multi-armed bandit”1, a classic model in reinforcement learning.\nUsing a gambling machine allows us to bring this model squarely into the realm of probabilities, and it adds a natural system of rewards, i.e. prizes. So say, for example, you walk into a casino and start playing 3 machines. You go from one to the other over the course of 6 turns and get the following prizes.\nWhat do you do next?\n\n\nCode\ndf = pd.DataFrame(\n    {\n        \"turn\": [1, 2, 3, 4, 5, 6],\n        \"machine_id\": [1, 2, 3, 1, 2, 3],\n        \"prize\": [0.10, 1.32, 0.29, 1.18, 1.10, 0.17]\n    }\n)\n\n# Format table\nd = dict(selector=\"th\",\n    props=[('text-align', 'center')])\n\n(df.style.hide()\n    .format({'prize': '${:,.2f}'})\n    .set_properties(**{'width':'10em', 'text-align':'center'})\n    .set_table_styles([d]))\n\n\n\n\n\n\nTable 1: Results from 6 Turns\n\n\nturn\nmachine_id\nprize\n\n\n\n\n1\n1\n$0.10\n\n\n2\n2\n$1.32\n\n\n3\n3\n$0.29\n\n\n4\n1\n$1.18\n\n\n5\n2\n$1.10\n\n\n6\n3\n$0.17\n\n\n\n\n\n\nMy set up for the model is as follows:\n\nWe are in a casino with 3 slot machines (one-armed bandits).\nWe take 500 turns, deciding which machine to play on each turn.\nIt costs $1.00 to play each turn.\nExpected reward can change over time, but in the long run is $0.99.\nThe reward on a given play is exponentially distributed around the expected value.\n\nModelling the prizes won by a player as an exponential distribution seems appropriate because it will give us many small wins and a few big wins. I thought it would also be good to use a challenging distribution. Models like these can be set up with normal distributions that have small standard deviations, but these are pretty easy to learn as a small standard deviation means you can more accurately guess the actual expected value for a machine. For the exponential distribution, the standard deviation is equal to the mean. Additionally, the skew of the distribution means there will be more curveball high valued prizes that have to be accounted for.\nBelow I have an example plot of the exponential distribution with an expected prize of $0.99, with some sample prizes scattered around that. As you can see, most of the turns will give you a prize less than $0.99, but some of the turns will give you much more.\n\n\nCode\n# Parameters\nrng = np.random.default_rng(7)\nmean = 0.99\nlambda_param = 1.0 / mean\nx = np.linspace(0, 6, 400)  # Generate x values\ny = lambda_param * np.exp(-lambda_param * x)  # Exponential distribution function\n\n# Sample 50 points and jitter for dodging\nsample_points_x = rng.exponential(mean, 50)\njitter = 0.05  # Adjust this value for more/less jitter\nsample_points_y = [rng.uniform(-jitter, jitter) for _ in range(50)]\n\npalette = sns.color_palette()\n\nsns.lineplot(x=x, y=y)\nplt.plot([mean, mean], [lambda_param * np.exp(-lambda_param * mean), 0], color=palette[0])\nplt.annotate(\"expected prize\", (mean, 0.2), (1.5, 0.5),\n    arrowprops=dict(facecolor=palette[0], edgecolor=palette[0],\n        arrowstyle=\"simple,tail_width=0.07,head_width=0.7,head_length=1\"))\nplt.annotate(\"random samples\", (2.5, 0), (4, 0.3),\n    arrowprops=dict(facecolor=palette[0], edgecolor=palette[0],\n        arrowstyle=\"simple,tail_width=0.07,head_width=0.7,head_length=1\"))\nsns.scatterplot(x=sample_points_x, y=sample_points_y)\n# plt.grid(axis='y', color='grey', linestyle='--', linewidth=0.5)\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.0f}')\nplt.gca().xaxis.set_major_formatter(formatter)\nplt.xlabel('prize')\nplt.ylabel('probability density function')\nplt.show()\n\n\n\n\n\nFigure 1: Example of an Exponential Distribution\n\n\n\n\nMachines with an identical distribution of random prizes are not interesting to model though, because there would be no “good” machine to pick. Whether you pick one machine or hop around, you would have the same expected prize. The actual prize you win will vary through random chance which is not predictable.\nIt’s also not that interesting if the expected value never changes. A static expected value would also limit the applicability of a model like this. In real life, a restaurant (say) will get better or worse over time. If we have a decision making model that doesn’t recognise that, then it’s not that useful in reality.\nInstead, we are interested in the cases where different machines have different expected outcomes, and they will each follow a random walk (with mean recursion). That random walk will look like this, noting that these are expected prizes, not actual prizes:\n\n\nCode\nn_iterations = 5000\nn_machines = 3\nn_turns = 500\n\nsigma = 0.2\ncost_per_game = 1\nexpected_prize = 0.99\nstarting_value = 100\n\nrng = np.random.default_rng(5)\n\nmean_lin = np.ones(n_machines) * expected_prize\nvar_lin = np.ones(n_machines) * (sigma**2)\n\nmean_log = np.log(mean_lin**2 / (np.sqrt(mean_lin**2 + var_lin)))\nvar_log = np.log(1 + var_lin / mean_lin**2) \ncov_log = np.diag(var_log)\n\n# Mean-reverting process\nnoise_log = rng.multivariate_normal(np.zeros(n_machines), cov_log, size=(n_iterations, n_turns + 200))\n\ne_rewards_log = np.ones_like(noise_log) * mean_log\ntheta = 0.01\nfor i in range(1, n_turns + 200):\n    e_rewards_log[:, i, :] = e_rewards_log[:, i - 1, :] + (\n        0.01 * (mean_log - e_rewards_log[:, i - 1, :]) + 0.15 * noise_log[:, i, :]\n        )\ne_rewards = np.exp(e_rewards_log[:, 200:])\n\n# Generate rewards from expected rewards\nrewards = rng.exponential(e_rewards)\n\nmachine_labels = [str(i + 1) for i in range(rewards.shape[2])]\n\nrewards_df = pd.DataFrame(rewards[0, :, :])\nrewards_df.columns = machine_labels\nrewards_df[\"turn\"] = list(range(1, rewards.shape[1] + 1))\nrewards_df = rewards_df.melt(\n    id_vars=\"turn\",\n    value_vars=machine_labels,\n    value_name=\"prize\",\n    var_name=\"machine\",)\n\ne_rewards_df = pd.DataFrame(e_rewards[0, ::])\ne_rewards_df.columns = machine_labels\ne_rewards_df[\"turn\"] = list(range(1, e_rewards.shape[1] + 1))\ne_rewards_df = e_rewards_df.melt(\n    id_vars=\"turn\",\n    value_vars=machine_labels,\n    value_name=\"expected_prize\",\n    var_name=\"machine\",)\n\nrewards_df = rewards_df.merge(e_rewards_df, on=(\"turn\", \"machine\"), how=\"left\")\n\n# Plot\nax = sns.lineplot(data=rewards_df, x=\"turn\", y=\"expected_prize\", hue=\"machine\")\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.ylim(0.6, 1.8)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n\n\n\n\n\nFigure 2: Expected Prizes for each of 3 Machines\n\n\n\n\nThe actual prizes you would win if you played all the machines at the same time would look as follows:\n\n\nCode\nax = sns.scatterplot(data=rewards_df, x=\"turn\", y=\"prize\", hue=\"machine\", s=7)\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.ylim(0.0, 11)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n\n\n\n\n\nFigure 3: Actual Rewards for each of 3 Machines"
  },
  {
    "objectID": "posts/one-armed-bandit/index.html#default-strategies",
    "href": "posts/one-armed-bandit/index.html#default-strategies",
    "title": "Decision-Making - Beating the Bandits",
    "section": "Default Strategies",
    "text": "Default Strategies\nIn order to know whether a strategy is good or bad, we need to create some benchmarks. Here we will use the Oracle strategy, which assumes we have perfect knowledge of the environment, and the Random strategy, which assumes we don’t know anything at all, and won’t try to learn anything either.\n\nThe Oracle Strategy\nUnder this strategy, we assume that we have some oracle that tells us what to expect from each machine, i.e. it knows which machine is the best to play on each turn. However, remember that the expectation is just a parameter of the distribution used to generate prize amounts. The actual prize is generated randomly, so we can’t know it in advance. So, even the oracle will not get the maximum prize on each turn.\nThe expected return to the oracle on each looks like this, i.e. it always plays the machine with the highest expected prize:\n\n\nCode\noracle_e_rewards = np.max(e_rewards[0], axis=1)\nturns = list(range(1, len(oracle_e_rewards)+1))\n\nax = sns.lineplot(data=rewards_df, x=\"turn\", y=\"expected_prize\", hue=\"machine\", alpha=0.3)\nsns.lineplot(x=turns, y=oracle_e_rewards, color=palette[4], label=\"oracle\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0.6, 1.8)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n\n\n\n\n\nFigure 4: Expected Prizes under the Oracle Strategy\n\n\n\n\n\n\nThe Random Strategy\nAnother strategy we could use is to pick a machine to play at random in each round. This might seem like an ok way to play the game, but remember, each game has a cost associated with it of $1.00, and an expected reward of $0.99. We should expect that a player following this strategy will not quite get back the money they spend on the machine.\n\n\nCode\nrandom_choice = rng.integers(0, n_machines, (n_turns))\nrandom_choice = np.expand_dims(random_choice, 1)\nrandom_e_rewards = np.take_along_axis(e_rewards[0], random_choice, axis=1)\nrandom_e_rewards = np.squeeze(random_e_rewards)\n\n# random_e_rewards = rng.choice(e_rewards[0], axis=1)\nturns = list(range(1, len(oracle_e_rewards)+1))\n\nax = sns.lineplot(data=rewards_df, x=\"turn\", y=\"expected_prize\", hue=\"machine\", alpha=0.3)\nsns.lineplot(x=turns, y=random_e_rewards, color=palette[5], label=\"random\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0.6, 1.8)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n\n\n\n\n\nFigure 5: Expected Prizes under the Random Strategy\n\n\n\n\nThe plots above all track a single game, where a player sits down and plays 500 games. To see how effective our strategies are, we will instead look at the average prize per turn in a Monte Carlo/Bootstrap simulation with 5000 iterations:\n\n\nCode\n# Oracle Strategy\noracle_choice = np.argmax(e_rewards, axis=2, keepdims=True)\noracle_rewards = np.take_along_axis(rewards, oracle_choice, axis=2)\noracle_rewards = np.squeeze(oracle_rewards)\n\nturns = np.array(list(range(1, n_turns+1)))\naverage_oracle_rewards = np.cumsum(oracle_rewards, axis=1) / turns\naverage_oracle_rewards = np.mean(average_oracle_rewards, axis=0)\n\n# Random Strategy\nrandom_choice = rng.integers(0, n_machines, (n_iterations, n_turns))\nrandom_choice = np.expand_dims(random_choice, 2)\nrandom_rewards = np.take_along_axis(rewards, random_choice, axis=2)\nrandom_rewards = np.squeeze(random_rewards)\n\nturns = np.array(list(range(1, random_rewards.shape[1]+1)))\naverage_random_rewards = np.cumsum(random_rewards, axis=1) / turns\naverage_random_rewards = np.mean(average_random_rewards, axis=0)\n\n# Plotting\nsns.lineplot(x=turns, y=average_oracle_rewards, color=palette[4], label=\"oracle\")\nsns.lineplot(x=turns, y=average_random_rewards, color=palette[5], label=\"random\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 0.98, 1])\nplt.ylim(0.6, 1.8)\nplt.xlabel(\"turn\")\nplt.ylabel(\"average_prize\")\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n\n\n\n\n\nFigure 6: Running Average of Prizes won by Oracle and Random Strategies over 5000 iterations\n\n\n\n\nAs expected, the random strategy just about fails to justify the $1.00 price to play the game. On average, it hands out a prize of just less than that."
  },
  {
    "objectID": "posts/one-armed-bandit/index.html#some-simple-strategies",
    "href": "posts/one-armed-bandit/index.html#some-simple-strategies",
    "title": "Decision-Making - Beating the Bandits",
    "section": "Some Simple Strategies",
    "text": "Some Simple Strategies\nNow, we will look at some simple rules that can be used to play the game.\n\nExploitation\nHere, “exploitation” refers to exploiting the knowledge that we have. The way this one will work is that we will play each machine once. After that, we will assume that the best machine we saw in the initial round will be the best for the rest of the game. Let’s see what happens when we stick to our guns.\n\n\nCode\ndef strategy(epsilon: float = 0.0, alpha: float=None, initial_value: float = None):\n    \"\"\"Runs a Strategy over a Bootstrap/Monte Carlo of rewards.\n\n    Inputs:\n    -------\n    epsilon: float\n        The probability that the user will try a different machine at random in a given game.\n    alpha: float\n        The step size parameter used for exponential weighted averaging. A value of None\n        means that the mean is used.\n    initial_value: float\n        The prize expected by the player for each machine at the beginning of the game.\n    \"\"\"\n    # Initialize vectors\n    running_value_estimate = np.zeros((n_iterations, n_turns, n_machines))\n    running_count = np.zeros((n_iterations, n_turns, n_machines), dtype=int)\n    strategy_rewards = np.zeros((n_iterations, n_turns))\n    running_selection = np.zeros((n_iterations, n_turns), dtype=int)\n    selection = np.zeros(n_iterations, dtype=int)[:, None]\n\n    # Instantiate all random variables up front\n    random_selection = rng.integers(low=0, high=n_machines, size=(n_iterations, n_turns))\n    random_explore = rng.uniform(0, 1, size=(n_iterations, n_turns))\n    for i in range(n_turns):\n        if i &lt; n_machines:\n            # Try all machines once.\n            selection = np.array([i]*n_iterations)[:, None]\n        else:\n            # Explore with some probability epsilon\n            explore = random_explore[:, i] &lt; epsilon\n            selection[explore] = random_selection[explore, i][:, None]\n            # Otherwise, use greedy selection (select machine thought most valuable)\n            selection[~explore] = np.argmax(running_value_estimate[~explore, i-1, :], axis=1)[:, None]\n\n        running_selection[:, i] = selection[:, 0]\n\n        strategy_rewards[:, i] = np.take_along_axis(rewards[:, i, :], selection, axis=1)[:, 0]\n\n        if i &gt; 0:\n            running_count[:, i, :] = running_count[:, i - 1, :]\n        update_count = np.zeros((n_iterations, n_machines))\n        np.put_along_axis(update_count, selection, 1, axis = 1)\n        running_count[:, i, :] = running_count[:, i, :] + update_count\n\n        if i &lt; n_machines and initial_value is None:\n            # If initial_value is None, start with initial value observed in machines.\n            # NOTE: initial iterations could be randomized, but iterating along machines\n            # 1, 2, 3, ... is random enough for this exercise.\n            np.put_along_axis(running_value_estimate[:, i, :], selection, strategy_rewards[:, i][:, None], axis=1)\n        else:\n            if i == 0 and initial_value is not None:\n                # If there is an initial_value, start with that.\n                running_value_estimate[:, i, :] = initial_value\n            else:\n                running_value_estimate[:, i, :] = running_value_estimate[:, i - 1, :]\n\n            if alpha is not None:\n                # Exponential Weight Decay\n                step_size = alpha\n            else:\n                # Incremental Mean Update\n                step_size = 1/np.take_along_axis(running_count[:, i, :], selection, axis=1) \n            \n            update_flat = (\n                step_size\n                * (\n                    strategy_rewards[:, i][:, None] \n                    - np.take_along_axis(running_value_estimate[:, i, :], selection, axis=1))\n            )\n            update = np.zeros((n_iterations, n_machines))\n            np.put_along_axis(update, selection, update_flat, axis=1)\n            running_value_estimate[:, i, :] = running_value_estimate[:, i, :] + update\n\n    return running_value_estimate, running_count, strategy_rewards, running_selection\n\nexploitation_results = strategy(epsilon=0.0)\naverage_exploitation_rewards = np.mean(exploitation_results[2], axis=0)\n# Plotting\nsns.lineplot(x=turns, y=average_exploitation_rewards, color=palette[6], label=\"exploitation\")\nsns.lineplot(x=turns, y=average_oracle_rewards, color=palette[4], label=\"oracle\")\nsns.lineplot(x=turns, y=average_random_rewards, color=palette[5], label=\"random\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0.9, 1.2)\nplt.xlabel(\"turn\")\nplt.ylabel(\"average_prize\")\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n\n\n\n\n\nFigure 7: Running Average of Prizes won by the Exploitation Strategy over 5000 iterations\n\n\n\n\nThis turns out to be a bad strategy, no better than random chance (in expectation) in our example. Remember, the actual prizes in this game are randomly generated from a distribution around the expected value. The player might be more likely to see a better prize from the better machine, but because they did not play the other machines again, they never found out if one of the others might have been better after all.\n\n\nExploration\nIn general, we will still want to exploit the information that we have found out about the machines in the game. We know that picking random machines on every turn is a strategy that will lose in the long term. But, we still need to check the other machines from time to see if we’ve maybe underestimated them. This leads us to the idea of exploration. With some probability, instead of picking the machine we think is best, we will instead pick one completely at random.\n\n\nCode\nexploratation_results = strategy(epsilon=0.2)\naverage_exploratation_rewards = np.mean(exploratation_results[2], axis=0)\n# Plotting\nsns.lineplot(x=turns, y=average_exploitation_rewards, color=palette[6], label=\"exploitation\", alpha = 0.3)\nsns.lineplot(x=turns, y=average_exploratation_rewards, color=palette[6], label=\"exploratation\")\nsns.lineplot(x=turns, y=average_oracle_rewards, color=palette[4], label=\"oracle\")\nsns.lineplot(x=turns, y=average_random_rewards, color=palette[5], label=\"random\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0.9, 1.2)\nplt.xlabel(\"turn\")\nplt.ylabel(\"average_prize\")\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n\n\n\n\n\nFigure 8: Running Average of Prizes won by the Exploration Strategy over 5000 iterations\n\n\n\n\nOkay! Now we’re starting to see some progress! We’ve hit on a way to finally start making money on these prizes (in expectation at least). But, there’s something weird going on, our success gets worse as the game goes on.\nWhat’s happening here is a result of the fact that the expected prize given out by the machine is not constant over time. In fact, over a long enough time frame, I’ve set the machines to vary around our expected prize value of $0.99. Because our rule takes the average over all turns of the machine, when we are on turn 1000, we are still weighing the prize we got on turn 1 the same as the prize we got on turn 999.\n\n\nRecency Bias\nIn order to combat this, what we want to do (in this particular situation) is to place more weight on recent observations. This will allow us to ride the wave when one machine is outperforming the others. We will do this using exponentially weighted averaging, with a parameter that weights recent observations just a little bit more highly than the average of all the observations we’ve seen so far. In this way, the ability of the early wins to influence our later decision making is reduced further and further with every turn.\n\n\nCode\nexp_weighting_results = strategy(epsilon=0.25, alpha=0.15)\naverage_exp_weighting_rewards = np.mean(exp_weighting_results[2], axis=0)\n# Plotting\nsns.lineplot(x=turns, y=average_exploitation_rewards, color=palette[6], label=\"exploitation\", alpha=0.3)\nsns.lineplot(x=turns, y=average_exploratation_rewards, color=palette[6], label=\"exploratation\", alpha=0.3)\nsns.lineplot(x=turns, y=average_exp_weighting_rewards, color=palette[6], label=\"exp_weighting\")\nsns.lineplot(x=turns, y=average_oracle_rewards, color=palette[4], label=\"oracle\")\nsns.lineplot(x=turns, y=average_random_rewards, color=palette[5], label=\"random\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0.9, 1.2)\nplt.xlabel(\"turn\")\nplt.ylabel(\"average_prize\")\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n\n\n\n\n\nFigure 9: Running Average of Prizes won by the Exploration Strategy with exponentially weighted averaging over 5000 iterations\n\n\n\n\nBetter again! But, we have a period at the start of the game where it takes us about 50 turns to “get up to speed”. Let’s see if we can do something about that.\n\n\nOptimistic Starting Conditions\nTo try and improve performance in the early turns, we will start be slightly optimistically assuming thae each machine will hand out an expected $1.10 on each round. this seems to work, but admittedly, it’s not as impressive as the other strategies.\n\n\nCode\noptimistic_results = strategy(epsilon=0.25, alpha=0.15, initial_value=1.1)\naverage_optimistic_rewards = np.mean(optimistic_results[2], axis=0)\n# Plotting\nsns.lineplot(x=turns, y=average_exploitation_rewards, color=palette[6], label=\"exploitation\", alpha=0.3)\nsns.lineplot(x=turns, y=average_exploratation_rewards, color=palette[6], label=\"exploratation\", alpha=0.3)\nsns.lineplot(x=turns, y=average_exp_weighting_rewards, color=palette[6], label=\"exp_weighting\", alpha=0.3)\nsns.lineplot(x=turns, y=average_optimistic_rewards, color=palette[6], label=\"optimistic\")\nsns.lineplot(x=turns, y=average_oracle_rewards, color=palette[4], label=\"oracle\")\nsns.lineplot(x=turns, y=average_random_rewards, color=palette[5], label=\"random\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0.9, 1.2)\nplt.xlabel(\"turn\")\nplt.ylabel(\"average_prize\")\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n\n\n\n\n\nFigure 10: Running Average of Prizes won by the Exploration Strategy with exponentially weighted averaging and optimistic initial expectations over 5000 iterations"
  },
  {
    "objectID": "posts/one-armed-bandit/index.html#a-note-on-risk",
    "href": "posts/one-armed-bandit/index.html#a-note-on-risk",
    "title": "Decision-Making - Beating the Bandits",
    "section": "A Note on Risk",
    "text": "A Note on Risk\nSo, above we look at the average return to each strategy. But the average is only one part of the story. To see how we might end up, we will model the paths of 5000 games, each through 500 turns. We can then easily create a plot to see how players are likely to finish up if they start with $100 and follow each of these strategies. The charts below are path dependent, so if the player runs out of money, they’re out of the game.\n\n\nCode\ndef final_stats(rewards: np.array, initial_holding: float=100, cost_per_play: float=1.0):\n    holdings = np.cumsum(rewards - np.ones(rewards.shape[1])[None], axis=1) + initial_holding \n    final_holdings = holdings[:, -1]\n    final_holdings[np.any(holdings&lt;=0, axis=1)] = 0\n\n    sorted_final_holdings = np.sort(final_holdings)\n    cdf_prob = np.arange(1, len(sorted_final_holdings) + 1) / len(sorted_final_holdings)\n    return sorted_final_holdings, cdf_prob\n\nstats_oracle = final_stats(oracle_rewards)\nstats_random = final_stats(random_rewards)\nstats_optimistic = final_stats(optimistic_results[2])\n\nplt.axvline(x=100, color=palette[0])\nsns.lineplot(x=stats_oracle[0], y=stats_oracle[1], color=palette[4], label=\"oracle\")\nsns.lineplot(x=stats_random[0], y=stats_random[1], color=palette[5], label=\"random\")\nsns.lineplot(x=stats_optimistic[0], y=stats_optimistic[1], color=palette[6], label=\"optimistic\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0, 1)\nplt.xlabel(\"final_prize_money\")\nplt.ylabel(\"continuous_density_function\")\n\nplt.annotate(\"breakeven\", (100, 0.1), (300, 0.18),\n    arrowprops=dict(facecolor=palette[0], edgecolor=palette[0],\n        arrowstyle=\"simple,tail_width=0.07,head_width=0.7,head_length=1\"))\nformatter_perc = ticker.FuncFormatter(lambda x, pos: f'{x:.0%}')\nformatter_dollar = ticker.FuncFormatter(lambda x, pos: f'${x:.0f}')\nplt.gca().yaxis.set_major_formatter(formatter_perc)\nplt.gca().xaxis.set_major_formatter(formatter_dollar)\nplt.show()\n\n\n\n\n\nFigure 11: Cumulative Density Function for 5000 iterations of Exploration Strategy with exponentially weighted averaging and optimistic initial expectations\n\n\n\n\nThe plots above tell us that we would likely have lost money over 55% of the time if we randomly pick a machine to play each time. Following the strategy we came up with with exploitation and exploration, using exponential weighting, and optimistic initial expectations we would have lost money less than 30% of the time. Due to randomness, even the oracle strategy would have resulted in losses about 5% of the time."
  },
  {
    "objectID": "posts/one-armed-bandit/index.html#summary",
    "href": "posts/one-armed-bandit/index.html#summary",
    "title": "Decision-Making - Beating the Bandits",
    "section": "Summary",
    "text": "Summary\nHopefully this was at least a little interesting and provided some useful insights into decision making. To recap, we looked a specific type of decision: a decision we make repeatedly, where we only see the reward we get for the choices we make. Under these conditions, we can use the concepts of exploration, a bias towards more recent observations, and an initially optimistic view of expectations. Operating under uncertainty, where you never get to know what the oracle strategy might be, concepts like these help us to maximize our return."
  },
  {
    "objectID": "posts/one-armed-bandit/index.html#further-reading",
    "href": "posts/one-armed-bandit/index.html#further-reading",
    "title": "Decision-Making - Beating the Bandits",
    "section": "Further Reading",
    "text": "Further Reading\nThere is a lot more to explore in reinforcement learning. First of all, there is the concept of a “state”. This allows us to incorporate more information into a decision, e.g. “I’d like Mexican food tonight” would make a difference in selecting a restaurant. There are also methods in machine learning to allow for planning, so you can make more complicated decisions, e.g. deciding which move to make next in a game of chess, and methods that allow us to learn by example, e.g. by watching someone else play chess.\nFor a more detailed exploration of the topic, I highly recommend the foundational Reinforcement Learning: An Introduction by Sutton and Barto."
  },
  {
    "objectID": "posts/one-armed-bandit/index.html#footnotes",
    "href": "posts/one-armed-bandit/index.html#footnotes",
    "title": "Decision-Making - Beating the Bandits",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe name comes from the fact that older mechanical slot machines had an arm on the side to make the machine work, and over the long run they will take all your money.↩︎"
  },
  {
    "objectID": "posts/ai-interface/index.html",
    "href": "posts/ai-interface/index.html",
    "title": "A New Intelligence, A New Interface",
    "section": "",
    "text": "This is a favorite topic of mine at the moment, based on how I see people using AI chat interfaces like ChatGPT. There seems to be too much you can do, and too few constraints, so in a kind of paradox of choice it becomes very hard for people to do anything useful."
  },
  {
    "objectID": "posts/ai-interface/index.html#the-terminal-why-dont-we-use-it",
    "href": "posts/ai-interface/index.html#the-terminal-why-dont-we-use-it",
    "title": "A New Intelligence, A New Interface",
    "section": "The Terminal: Why Don’t We Use It?",
    "text": "The Terminal: Why Don’t We Use It?\nI remember being a kid in the days where you still had to be able to navigate DOS to get PC games to play. There was lots of dir, cd directory, dir, program.exe until you were eventually rewarded with a cascade of text and the first graphics introducing the start of the game.\nThe terminal still exists today, but regular users rarely even see it, and probably wouldn’t know the first thing about how to interact with it. Anyone in a tech role recognizes that this black screen with a blinking cursor gives you access to everything, in a much deeper way than a gui ever can.\nIf it is so powerful, why doesn’t everyone use it regularly? Well, you kind of have to know a long list of incantations to make it do what you want. Everything has to be precisely coded in, in a very precise way. The basics of navigation require ls and cd, making and deleting directories has mkdir, rm and rm -rf. That’s 4 (and a half) commands you have to just know for basic tasks that are “obvious” in a GUI.\nOrdinary users moved away from the terminal because they don’t have to learn the incantations for every task they might want to do. GUI’s present everything to the user, with nicely signposted happy path that you just have to click on. A GUI actually presents a tradeoff, where we actually give up a lot of funcionality and optionality in the software we use in order to get something that does what we want with a lot less mental overhead."
  },
  {
    "objectID": "posts/ai-interface/index.html#paper-infinite-possibilities",
    "href": "posts/ai-interface/index.html#paper-infinite-possibilities",
    "title": "A New Intelligence, A New Interface",
    "section": "Paper: Infinite Possibilities",
    "text": "Paper: Infinite Possibilities\nTaking the terminal to the extreme, we could look at paper. Basically every work of literature, philosophy and discovery was written on physical paper up to just a few years ago. Paper and pen holds the nearly infinite capacity to record ideas, so potentially anyone can create any idea at any time on a piece of paper.\nA terminal has a limit in that it can only run the programs that are installed on the computer. A piece of paper can take you in any direction, in any language, with pictures, diagrams or words. However, the lack of constraints means that you don’t just have to know incantations that give you shortcuts to complete tasks, you need to actually understand how to complete the entire task yourself."
  },
  {
    "objectID": "posts/ai-interface/index.html#ai-beyond-the-chat-interface",
    "href": "posts/ai-interface/index.html#ai-beyond-the-chat-interface",
    "title": "A New Intelligence, A New Interface",
    "section": "AI: Beyond the Chat Interface",
    "text": "AI: Beyond the Chat Interface\nSo, it seems that there is a trade-off here:\n\nTerminal: great freedom, but need to understand terminal commands as shortcuts to complete more complicated tasks.\nGUI: limited freedom, but often don’t need to know anything at all to achieve an outcome.\n\nAI is generally at a point where, like a terminal, we are greeted by a flashing cursor, waiting for us to pilot it through a series of steps to complete a task. It is extremely powerful at taking our inputs and acting on them. However, there is a difficulty in knowing what the AI is capable of doing, and how to phrase your requests so that it actually does them. This can be seen in ChatGPT, where you need to know that, say, ChatGPT can generate pictures, and also that to do it you need to ask it to “make me a picture of..”.\nThere will be a new generation of AI apps that will be more GUI-like, and users will gravitate to them. Somehow, apps will be designed to drive the conversation, limiting the power of AI to do anything, but giving regular, non-ai-experts a greater ability to do something."
  },
  {
    "objectID": "posts/ai-interface/index.html#solutions",
    "href": "posts/ai-interface/index.html#solutions",
    "title": "A New Intelligence, A New Interface",
    "section": "Solutions",
    "text": "Solutions\nThe solution is to cut down on the number of options presented to users, and create “opinionated” software. Here’s a few ideas, ranging from more active user engagement to more passive:\n\nGUIs: where we try to cut down the infinite possibilities to the point where a regular user can do something interesting with a couple of buttons.\nFlipping the driving seat: so that instead of users coming up with the ideas, the AI is the one asking questions and driving the conversation.\nIntent mapping: instead of having the user directly engage with the AI, the AI monitors their activity and behaviour, and tries to infer next best actions.\nFilters: instead of actively engaging a user, the AI sits in the background selectively allowing or filtering information from the user.\n\nUltimate, there is a lot more work to do here. I don’t think we’ve yet found the form factor that makes most sense for AI."
  },
  {
    "objectID": "posts/fasthtml_tailwind/index.html",
    "href": "posts/fasthtml_tailwind/index.html",
    "title": "FastHTML and Tailwind",
    "section": "",
    "text": "If you like python and want to do web development and haven’t checked out FastHTML you need to stop reading this right now and go to fastht.ml right now.\nIt comes bundled with Pico CSS by default, an awesome, lightweight CSS framework. If you’ve done much frontend development though, you might just be reaching for tailwind CSS as you start really tying an app together."
  },
  {
    "objectID": "posts/fasthtml_tailwind/index.html#html-and-css",
    "href": "posts/fasthtml_tailwind/index.html#html-and-css",
    "title": "FastHTML and Tailwind",
    "section": "HTML and CSS",
    "text": "HTML and CSS\nAs a quick refresher to anyone who doesn’t do much web development, css is the language that tells a browser how to format html. This is the most basic html project I can think of, which just prints a header to the browser with some basic formatting.\nmy-project/\n├── index.html\n└── styles.css\n\n\nindex.html\n\n&lt;!DOCTYPE html&gt;\n&lt;head&gt;\n    &lt;title&gt;My Minimal HTML&lt;/title&gt;\n    &lt;link rel=\"stylesheet\" href=\"styles.css\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Hello, World!&lt;/h1&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\n\nstyles.css\n\nh1 {\n  color: blue;\n  font-size: 18px;\n}\n\nAbove, we have just one class, “H1”, but it can get unwieldy to define and maintain classes for all of the elements you put in a webapp. Think of all the types of text, buttons, labels, boxes, forms, images and more - these will all have css styles attached to them."
  },
  {
    "objectID": "posts/fasthtml_tailwind/index.html#tailwind-css",
    "href": "posts/fasthtml_tailwind/index.html#tailwind-css",
    "title": "FastHTML and Tailwind",
    "section": "Tailwind CSS",
    "text": "Tailwind CSS\nAn alternative to the traditional approach above is to use a utility-first framework like Tailwind CSS. Tailwind allows you to define the styling in the html in a highly compact way, like this:\nmy-project/\n└── index.html\n\n\nindex.html\n\n&lt;!DOCTYPE html&gt;\n&lt;head&gt;\n    &lt;title&gt;My Minimal HTML&lt;/title&gt;\n    &lt;script src=\"https://cdn.tailwindcss.com\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1 class=\"text-blue-500 text-lg\"&gt;Hello, World!&lt;/h1&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\nThis leads to just one file, where Tailwind CSS is downloaded directly from the CDN. The tailwind script automatically scans the html, figures out the full css for each Tailwind class, and adds (injects) the styling to the html with &lt;style&gt; tags (another way to add css to html). So inside the &lt;head&gt; tags you might have something like this:\n&lt;style&gt;\n.text-blue-500 { color: #3b82f6; }\n.text-lg { font-size: 1.125rem; }\n/* and so on... */\n&lt;/style&gt;\nIt might not seem like you gain much here, but tailwind gets extremely powerful as the styling starts to become more complicated.\n\nCustomization\nHowever, tailwind is highly customizable (through a file you create in your project tailwind.config.js) and this customization can’t be done the way we included the tailwind script from the CDN. Instead, we will have to run tailwind on our code to generate the CSS file that will be used to render the webpage."
  },
  {
    "objectID": "posts/fasthtml_tailwind/index.html#tailwind_installation",
    "href": "posts/fasthtml_tailwind/index.html#tailwind_installation",
    "title": "FastHTML and Tailwind",
    "section": "Tailwind Installation",
    "text": "Tailwind Installation\nUsually, tailwind is called while building a web app in javascript. However, we’re using python so we’ll have to do things a little more manually. For this, tailwind has a CLI.\nYou can download and install the CLI directly, but I prefer to use package managers; they keep things up to date, and also keep a list of what I’ve installed so I can go back later and remove them if I want. Usually, I’d use homebrew (I use a mac), but I can’t see the homebrew formula referenced on the official tailwind website. So instead I’m just going to use npm to install it globally on my machine with:\nnpm install -g tailwindcss\nInstalling it globally means it will be available to all your projects."
  },
  {
    "objectID": "posts/fasthtml_tailwind/index.html#initialise_project",
    "href": "posts/fasthtml_tailwind/index.html#initialise_project",
    "title": "FastHTML and Tailwind",
    "section": "Setting up a FastHTML project",
    "text": "Setting up a FastHTML project\nOk, let’s take a minute to set up a FastHTML project.\nI use the pdm package manager. This will set up my virtual environment and a basic file structure. I would initialize the project like this (I decided to call it basic_website), following all the default settings, starting in my projects folder:\nmkdir basic_website\ncd basic_website\npdm init\nmkdir static src/css src/app.py\nMy directory now looks like this. Running pdm has set up me up with the directory structure, the .gitignore file, the pyproject.toml file and a basic README.md.\nbasic_website\n├── .gitignore\n├── .pdm-python\n├── .venv\n│   ├── bin\n│   |   ├── activate\n│   |   ...\n│   ...\n├── README.md\n├── pyproject.toml\n├── src\n│   ├── app.py\n│   ├── basic_website\n│   |   └── __init__.py\n|   └── css   \n├── static\n└── tests\n    └── __init__.py\nYou can then add FastHTML with:\npdm add python-fasthtml\nYou might have to activate the virtual environment (often editors like vscode will detect them automatically). You can do that with:\nsource .venv/bin/activate\n\nStarting a Basic Webpage\nIn the src folder, I made app.py here like so:\n\n\nsrc/app.py\n\nimport fasthtml.common as fh\n\napp, rt = fh.FastHTML()\nrt = app.route\n\n\n@rt(\"/\")\ndef get():\n    return fh.Div(fh.P(\"Hello World!\"), hx_get=\"/change\")\n\n\nfh.serve()\n\nYou can run this with:\npython src/app.py\nAnd you can see your new webpage in your browser if you go to http://localhost:5001\nAwesome!\n\n\nInput Custom CSS\nIn addition to the tailwind classes you use, you’ll probably also want to add some custom css styling. I’m going to call mine main.css and put it in src/css.\nTo include tailwinds utility classes, we have to put this at the top of the css file:\n\n\nsrc/css/main.css\n\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n\nI’m going to add css that makes the text multi-colored by adding a “multicolor-text” class:\n\n\nsrc/css/main.css\n\n.multicolor-text {\n  display: inline-block;\n  background: linear-gradient(to left, blue, green);\n  background-clip: text;\n  color: transparent;\n}\n\n\n\nTailwind\n\nInitialization\nIn the directory where you’re writing your html, you can initialize tailwind with:\ntailwind init\nWhich creates tailwind.config.js, the file that will contain your customizations. It will set up the structure of the file, but you will need to at least add the types of files to scan (including .py files) like this:\n\n\ntailwind.config.js\n\n/** @type {import('tailwindcss').Config} */\nmodule.exports = {\n  content: [\n    \"./**/*.{py,html,qmd,md,js,jsx,ts,tsx,css}\", // Scan all relevant files\n  ],\n  theme: {\n    extend: {},\n  },\n  plugins: [],\n};\n\nNow, tailwind will detect each of these filetypes when it crawls through the directory, and turn any tailwind classes it sees into css.\n\n\nRunning Tailwind\nNow that we’ve set up tailwind, we will have to run it using the CLI, like this, specifying our input css file and our output css file (which we will reference in our FastHTML project).\ntailwind -i ./src/css/main.css -o ./static/main.css\nIt’s a bit of a pain to run this every time you update your css, so you can set it to run continuously (i.e. keep your terminal open) by adding the watch flag:\ntailwind -i ./src/css/main.css -o ./static/main.css -w\nWhen you’re distributing it for production, you can minify the css to make it more compact by adding the minify flag:\ntailwind -i ./src/css/main.css -o ./static/main.css -m\n\n\n\nFastHTML but with Tailwind\nNow, I have to add 2 things to my app.py, the link to the stylesheet, and this code which allows FastHTML to look for main.css in my static folder:\n\n\nsrc/app.py\n\nimport fasthtml.common as fh\n\n1main_css = fh.Link(rel=\"stylesheet\", href=\"main.css\", type=\"text/css\")\n\napp = fh.FastHTML(hdrs=(main_css))\nrt = app.route\n\n\n2@rt(\"/{fname:path}.{ext:static}\")\ndef get(fname: str, ext: str):\n    return fh.FileResponse(f\"static/{fname}.{ext}\")\n\n\n@rt(\"/\")\ndef get():\n3    return fh.Div(fh.H1(\"Hello World!\", cls=\"multicolor-text text-[50px]\"))\n\n\nfh.serve()\n\n\n1\n\nAdd the css file to the app.\n\n2\n\nAllow FastHTML to look in the static folder (where tailwind puts the css file).\n\n3\n\nActually use css to style the text, applying the custom multicolor-text class from our main.css file, and the tailwind text-[50px] class to set the text to 50px."
  },
  {
    "objectID": "posts/fasthtml_tailwind/index.html#result",
    "href": "posts/fasthtml_tailwind/index.html#result",
    "title": "FastHTML and Tailwind",
    "section": "Result",
    "text": "Result"
  },
  {
    "objectID": "posts/fasthtml_tailwind/index.html#summary",
    "href": "posts/fasthtml_tailwind/index.html#summary",
    "title": "FastHTML and Tailwind",
    "section": "Summary",
    "text": "Summary\nLet’s recap:\n\nTo use tailwind, you will have to install it. I installed it globally above.\nThen you initialize your project, creating the FastHTML app.\nAdd a custom css file and run tailwind.\nAdd the output css file to your FastHTML app."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Drop the BAS\n\n\nLearning: the Precursor to Ideas\n\n\n\n\nproduct\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2024\n\n\nSean Daly\n\n\n\n\n\n\n  \n\n\n\n\nA New Intelligence, A New Interface\n\n\nGoing past the Terminal (Again)\n\n\n\n\nai\n\n\ndesign\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2024\n\n\nSean Daly\n\n\n\n\n\n\n  \n\n\n\n\nFastHTML and Tailwind\n\n\nA how-to guide on how to get Tailwind working with FastHTML\n\n\n\n\nfasthtml\n\n\napp-development\n\n\n\n\n\n\n\n\n\n\n\nAug 26, 2024\n\n\nSean Daly\n\n\n\n\n\n\n  \n\n\n\n\nFinding Markov\n\n\nFitting Hidden Markov Models\n\n\n\n\nsimulation\n\n\ntail-risk\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2023\n\n\nSean Daly\n\n\n\n\n\n\n  \n\n\n\n\nDecision-Making - Beating the Bandits\n\n\n\n\n\n\n\ndecision-making\n\n\nreinforcement-learning\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2023\n\n\nSean Daly\n\n\n\n\n\n\nNo matching items"
  }
]