{
  "hash": "1731e672fb6d40232e8324a3e2c7a519",
  "result": {
    "markdown": "---\ntitle: \"Decision-Making - Beating the Bandits\"\nauthor: \"Sean Daly\"\ndate: \"2023-08-03\"\nimage: \"one-armed-bandits.png\"\ndraft: false\ntoc: true\nformat:\n    html:\n        code-fold: true\ncategories:\n  - decision-making\n  - reinforcement-learning\n---\n\n\n<!-- ![](man-machines.jpeg){fig-align=\"center\"} -->\n\nDecisions are hard. We can't see how things will turn out, and we never have all the information we need to make the best decision right now. However, there are systematic ways to work through the choices we make to give us a better chance of coming out on top.\n\nOne way to view things is through the lens of reinforcement learning. This is a flexible framework for decision making that assumes there is some environment out there to act on, and whatever we decide to do the environment gives us some feedback in the form of a reward or a punishment. Through these interactions, we can learn a better decision making rule.\n\nThere is a little more to the framework, such as the idea of a \"state\" that can impact the results of our actions, and there are diferent ways of setting up a model for analysis. But, we will ignore that for now for the classic model I will talk about below: the multi-armed bandit. The way I set this up allows for the following types of decisions:\n\n- We face a repeated decision between a number of options, and can choose only one on each turn. There are many turns.\n- We don't know the expected value of each choice. We only find out the reward we get for the option we choose, and the only information we have is the record of choices we made and the rewards we got for those choices.\n- The expected value of each option can change over time, and the actual reward we get for choosing an option is noisy (i.e. randomly distributed around the expected value), i.e. good options sometimes provide bad results.\n\nYou could apply the above to decisions such as:\n\n- Which restaurant/barbershop should I go to?\n- Which vendor should I buy from?\n- Who is the best person to send into a sales pitch?\n\nIn all the above, we have some number of options, and we keep gong back to make these decisions repeatedly. Each option can generally get better or worse over time as staff get more experienced or leave, but we generally don't know what impact that will have until we get to see how it plays out, and each one of them can have days where they uncharacteristically knock it out of the park or just do a terrible job.\n\nBesides being actually useful, this type of decision is also nice because it's relatively simple and it demonstrates some really nice elements of a decision-making strategy, which are the main takeaways from this post:\n\n> - **Exploration v. Exploitation**: a really key concept to grasp. Do you explore new restaurants every weekend like an epicurean nomad, or do you decide you've found one that's good enough (exploiting what you already know)? Spoiler: you try to balance both.\n> - **Bias Towards Recency**: Things change all the time - you should change with them.\n> - **Optimistic Initial Expectations**: Believe it or not, until you get some experience you're better off assuming the best.\n\n## The Multi-Armed Bandit Model\n\nThe actual model we will look at to demonstrate the idea is called the \"multi-armed bandit\"[^1], a classic model in reinforcement learning.\n\n[^1]: The name comes from the fact that older mechanical slot machines had an arm on the side to make the machine work, and over the long run they will take all your money.\n\nUsing a gambling machine allows us to bring this model squarely into the realm of probabilities, and it adds a natural system of rewards, i.e. prizes. So say, for example, you walk into a casino and start playing 3 machines. You go from one to the other over the course of 6 turns and get the following prizes. \n\nWhat do you do next?\n\n::: {#tbl-6_turns .cell tbl-cap='Results from 6 Turns' execution_count=2}\n``` {.python .cell-code}\ndf = pd.DataFrame(\n    {\n        \"turn\": [1, 2, 3, 4, 5, 6],\n        \"machine_id\": [1, 2, 3, 1, 2, 3],\n        \"prize\": [0.10, 1.32, 0.29, 1.18, 1.10, 0.17]\n    }\n)\n\n# Format table\nd = dict(selector=\"th\",\n    props=[('text-align', 'center')])\n\n(df.style.hide()\n    .format({'prize': '${:,.2f}'})\n    .set_properties(**{'width':'10em', 'text-align':'center'})\n    .set_table_styles([d]))\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<style type=\"text/css\">\n#T_683c8 th {\n  text-align: center;\n}\n#T_683c8_row0_col0, #T_683c8_row0_col1, #T_683c8_row0_col2, #T_683c8_row1_col0, #T_683c8_row1_col1, #T_683c8_row1_col2, #T_683c8_row2_col0, #T_683c8_row2_col1, #T_683c8_row2_col2, #T_683c8_row3_col0, #T_683c8_row3_col1, #T_683c8_row3_col2, #T_683c8_row4_col0, #T_683c8_row4_col1, #T_683c8_row4_col2, #T_683c8_row5_col0, #T_683c8_row5_col1, #T_683c8_row5_col2 {\n  width: 10em;\n  text-align: center;\n}\n</style>\n<table id=\"T_683c8\">\n  <thead>\n    <tr>\n      <th id=\"T_683c8_level0_col0\" class=\"col_heading level0 col0\" >turn</th>\n      <th id=\"T_683c8_level0_col1\" class=\"col_heading level0 col1\" >machine_id</th>\n      <th id=\"T_683c8_level0_col2\" class=\"col_heading level0 col2\" >prize</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_683c8_row0_col0\" class=\"data row0 col0\" >1</td>\n      <td id=\"T_683c8_row0_col1\" class=\"data row0 col1\" >1</td>\n      <td id=\"T_683c8_row0_col2\" class=\"data row0 col2\" >$0.10</td>\n    </tr>\n    <tr>\n      <td id=\"T_683c8_row1_col0\" class=\"data row1 col0\" >2</td>\n      <td id=\"T_683c8_row1_col1\" class=\"data row1 col1\" >2</td>\n      <td id=\"T_683c8_row1_col2\" class=\"data row1 col2\" >$1.32</td>\n    </tr>\n    <tr>\n      <td id=\"T_683c8_row2_col0\" class=\"data row2 col0\" >3</td>\n      <td id=\"T_683c8_row2_col1\" class=\"data row2 col1\" >3</td>\n      <td id=\"T_683c8_row2_col2\" class=\"data row2 col2\" >$0.29</td>\n    </tr>\n    <tr>\n      <td id=\"T_683c8_row3_col0\" class=\"data row3 col0\" >4</td>\n      <td id=\"T_683c8_row3_col1\" class=\"data row3 col1\" >1</td>\n      <td id=\"T_683c8_row3_col2\" class=\"data row3 col2\" >$1.18</td>\n    </tr>\n    <tr>\n      <td id=\"T_683c8_row4_col0\" class=\"data row4 col0\" >5</td>\n      <td id=\"T_683c8_row4_col1\" class=\"data row4 col1\" >2</td>\n      <td id=\"T_683c8_row4_col2\" class=\"data row4 col2\" >$1.10</td>\n    </tr>\n    <tr>\n      <td id=\"T_683c8_row5_col0\" class=\"data row5 col0\" >6</td>\n      <td id=\"T_683c8_row5_col1\" class=\"data row5 col1\" >3</td>\n      <td id=\"T_683c8_row5_col2\" class=\"data row5 col2\" >$0.17</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nMy set up for the model is as follows:\n\n- We are in a casino with 3 slot machines (one-armed bandits).\n- We take 500 turns, deciding which machine to play on each turn.\n- It costs $1.00 to play each turn.\n- Expected reward can change over time, but in the long run is $0.99.\n- The reward on a given play is exponentially distributed around the expected value.\n\nModelling the prizes won by a player as an exponential distribution seems appropriate because it will give us many small wins and a few big wins. I thought it would also be good to use a challenging distribution. Models like these can be set up with normal distributions that have small standard deviations, but these are pretty easy to learn as a small standard deviation means you can more accurately guess the actual expected value for a machine. For the exponential distribution, the standard deviation is equal to the mean. Additionally, the skew of the distribution means there will be more curveball high valued prizes that have to be accounted for.\n\nBelow I have an example plot of the exponential distribution with an expected prize of $0.99, with some sample prizes scattered around that. As you can see, most of the turns will give you a prize less than $0.99, but some of the turns will give you much more.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Parameters\nrng = np.random.default_rng(7)\nmean = 0.99\nlambda_param = 1.0 / mean\nx = np.linspace(0, 6, 400)  # Generate x values\ny = lambda_param * np.exp(-lambda_param * x)  # Exponential distribution function\n\n# Sample 50 points and jitter for dodging\nsample_points_x = rng.exponential(mean, 50)\njitter = 0.05  # Adjust this value for more/less jitter\nsample_points_y = [rng.uniform(-jitter, jitter) for _ in range(50)]\n\npalette = sns.color_palette()\n\nsns.lineplot(x=x, y=y)\nplt.plot([mean, mean], [lambda_param * np.exp(-lambda_param * mean), 0], color=palette[0])\nplt.annotate(\"expected prize\", (mean, 0.2), (1.5, 0.5),\n    arrowprops=dict(facecolor=palette[0], edgecolor=palette[0],\n        arrowstyle=\"simple,tail_width=0.07,head_width=0.7,head_length=1\"))\nplt.annotate(\"random samples\", (2.5, 0), (4, 0.3),\n    arrowprops=dict(facecolor=palette[0], edgecolor=palette[0],\n        arrowstyle=\"simple,tail_width=0.07,head_width=0.7,head_length=1\"))\nsns.scatterplot(x=sample_points_x, y=sample_points_y)\n# plt.grid(axis='y', color='grey', linestyle='--', linewidth=0.5)\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.0f}')\nplt.gca().xaxis.set_major_formatter(formatter)\nplt.xlabel('prize')\nplt.ylabel('probability density function')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Example of an Exponential Distribution](index_files/figure-html/fig-exponential_distribution_pdf-output-1.png){#fig-exponential_distribution_pdf width=589 height=429 fig-alt='A plot of the exponential distribution probability density overlain by some sample points.'}\n:::\n:::\n\n\nMachines with an identical distribution of random prizes are not interesting to model though, because there would be no \"good\" machine to pick. Whether you pick one machine or hop around, you would have the same expected prize. The actual prize you win will vary through random chance which is not predictable.\n\nIt's also not that interesting if the expected value never changes. A static expected value would also limit the applicability of a model like this. In real life, a restaurant (say) will get better or worse over time. If we have a decision making model that doesn't recognise that, then it's not that useful in reality.\n\nInstead, we are interested in the cases where different machines have different expected outcomes, and they will each follow a random walk (with mean recursion). That random walk will look like this, noting that these are *expected* prizes, not actual prizes:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nn_iterations = 5000\nn_machines = 3\nn_turns = 500\n\nsigma = 0.2\ncost_per_game = 1\nexpected_prize = 0.99\nstarting_value = 100\n\nrng = np.random.default_rng(5)\n\nmean_lin = np.ones(n_machines) * expected_prize\nvar_lin = np.ones(n_machines) * (sigma**2)\n\nmean_log = np.log(mean_lin**2 / (np.sqrt(mean_lin**2 + var_lin)))\nvar_log = np.log(1 + var_lin / mean_lin**2) \ncov_log = np.diag(var_log)\n\n# Mean-reverting process\nnoise_log = rng.multivariate_normal(np.zeros(n_machines), cov_log, size=(n_iterations, n_turns + 200))\n\ne_rewards_log = np.ones_like(noise_log) * mean_log\ntheta = 0.01\nfor i in range(1, n_turns + 200):\n    e_rewards_log[:, i, :] = e_rewards_log[:, i - 1, :] + (\n        0.01 * (mean_log - e_rewards_log[:, i - 1, :]) + 0.15 * noise_log[:, i, :]\n        )\ne_rewards = np.exp(e_rewards_log[:, 200:])\n\n# Generate rewards from expected rewards\nrewards = rng.exponential(e_rewards)\n\nmachine_labels = [str(i + 1) for i in range(rewards.shape[2])]\n\nrewards_df = pd.DataFrame(rewards[0, :, :])\nrewards_df.columns = machine_labels\nrewards_df[\"turn\"] = list(range(1, rewards.shape[1] + 1))\nrewards_df = rewards_df.melt(\n    id_vars=\"turn\",\n    value_vars=machine_labels,\n    value_name=\"prize\",\n    var_name=\"machine\",)\n\ne_rewards_df = pd.DataFrame(e_rewards[0, ::])\ne_rewards_df.columns = machine_labels\ne_rewards_df[\"turn\"] = list(range(1, e_rewards.shape[1] + 1))\ne_rewards_df = e_rewards_df.melt(\n    id_vars=\"turn\",\n    value_vars=machine_labels,\n    value_name=\"expected_prize\",\n    var_name=\"machine\",)\n\nrewards_df = rewards_df.merge(e_rewards_df, on=(\"turn\", \"machine\"), how=\"left\")\n\n# Plot\nax = sns.lineplot(data=rewards_df, x=\"turn\", y=\"expected_prize\", hue=\"machine\")\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.ylim(0.6, 1.8)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Expected Prizes for each of 3 Machines](index_files/figure-html/fig-expected_reward-output-1.png){#fig-expected_reward width=669 height=434 fig-alt='A line-plot of the expected prizes to each of 3 machines.'}\n:::\n:::\n\n\nThe actual prizes you would win if you played all the machines at the same time would look as follows:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nax = sns.scatterplot(data=rewards_df, x=\"turn\", y=\"prize\", hue=\"machine\", s=7)\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.ylim(0.0, 11)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Actual Rewards for each of 3 Machines](index_files/figure-html/fig-reward-output-1.png){#fig-reward width=677 height=429 fig-alt='A scatter plot of the rewards to each of 3 machines.'}\n:::\n:::\n\n\n## Default Strategies\n\nIn order to know whether a strategy is good or bad, we need to create some benchmarks. Here we will use the Oracle strategy, which assumes we have perfect knowledge of the environment, and the Random strategy, which assumes we don't know anything at all, and won't try to learn anything either.\n\n### The Oracle Strategy\n\nUnder this strategy, we assume that we have some oracle that tells us what to expect from each machine, i.e. it knows which machine is the best to play on each turn. However, remember that the expectation is just a parameter of the distribution used to generate prize amounts. The actual prize is generated randomly, so we can't know it in advance. So, even the oracle will not get the maximum prize on each turn. \n\nThe expected return to the oracle on each looks like this, i.e. it always plays the machine with the highest expected prize:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\noracle_e_rewards = np.max(e_rewards[0], axis=1)\nturns = list(range(1, len(oracle_e_rewards)+1))\n\nax = sns.lineplot(data=rewards_df, x=\"turn\", y=\"expected_prize\", hue=\"machine\", alpha=0.3)\nsns.lineplot(x=turns, y=oracle_e_rewards, color=palette[4], label=\"oracle\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0.6, 1.8)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Expected Prizes under the Oracle Strategy](index_files/figure-html/fig-expected_reward_oracle-output-1.png){#fig-expected_reward_oracle width=679 height=475 fig-alt='A scatter plot of the prizes won by the Oracle Strategy.'}\n:::\n:::\n\n\n### The Random Strategy\n\nAnother strategy we could use is to pick a machine to play at random in each round. This might seem like an ok way to play the game, but remember, each game has a cost associated with it of $1.00, and an expected reward of $0.99. We should expect that a player following this strategy will not quite get back the money they spend on the machine.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nrandom_choice = rng.integers(0, n_machines, (n_turns))\nrandom_choice = np.expand_dims(random_choice, 1)\nrandom_e_rewards = np.take_along_axis(e_rewards[0], random_choice, axis=1)\nrandom_e_rewards = np.squeeze(random_e_rewards)\n\n# random_e_rewards = rng.choice(e_rewards[0], axis=1)\nturns = list(range(1, len(oracle_e_rewards)+1))\n\nax = sns.lineplot(data=rewards_df, x=\"turn\", y=\"expected_prize\", hue=\"machine\", alpha=0.3)\nsns.lineplot(x=turns, y=random_e_rewards, color=palette[5], label=\"random\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0.6, 1.8)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Expected Prizes under the Random Strategy](index_files/figure-html/fig-expected_reward_random-output-1.png){#fig-expected_reward_random width=679 height=475 fig-alt='A scatter plot of the prizes won by the Random Strategy.'}\n:::\n:::\n\n\nThe plots above all track a single game, where a player sits down and plays 500 games. To see how effective our strategies are, we will instead look at the average prize per turn in a Monte Carlo/Bootstrap simulation with 5000 iterations:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Oracle Strategy\noracle_choice = np.argmax(e_rewards, axis=2, keepdims=True)\noracle_rewards = np.take_along_axis(rewards, oracle_choice, axis=2)\noracle_rewards = np.squeeze(oracle_rewards)\n\nturns = np.array(list(range(1, n_turns+1)))\naverage_oracle_rewards = np.cumsum(oracle_rewards, axis=1) / turns\naverage_oracle_rewards = np.mean(average_oracle_rewards, axis=0)\n\n# Random Strategy\nrandom_choice = rng.integers(0, n_machines, (n_iterations, n_turns))\nrandom_choice = np.expand_dims(random_choice, 2)\nrandom_rewards = np.take_along_axis(rewards, random_choice, axis=2)\nrandom_rewards = np.squeeze(random_rewards)\n\nturns = np.array(list(range(1, random_rewards.shape[1]+1)))\naverage_random_rewards = np.cumsum(random_rewards, axis=1) / turns\naverage_random_rewards = np.mean(average_random_rewards, axis=0)\n\n# Plotting\nsns.lineplot(x=turns, y=average_oracle_rewards, color=palette[4], label=\"oracle\")\nsns.lineplot(x=turns, y=average_random_rewards, color=palette[5], label=\"random\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 0.98, 1])\nplt.ylim(0.6, 1.8)\nplt.xlabel(\"turn\")\nplt.ylabel(\"average_prize\")\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Running Average of Prizes won by Oracle and Random Strategies over 5000 iterations](index_files/figure-html/fig-average_reward_random-output-1.png){#fig-average_reward_random width=666 height=494 fig-alt='Line plot of the running Average of prizes won by Oracle and Random Strategies over 5000 iterations.'}\n:::\n:::\n\n\nAs expected, the random strategy just about fails to justify the $1.00 price to play the game. On average, it hands out a prize of just less than that.\n\n## Some Simple Strategies\n\nNow, we will look at some simple rules that can be used to play the game.\n\n### Exploitation\n\nHere, \"exploitation\" refers to exploiting the knowledge that we have. The way this one will work is that we will play each machine once. After that, we will assume that the best machine we saw in the initial round will be the best for the rest of the game. Let's see what happens when we stick to our guns.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndef strategy(epsilon: float = 0.0, alpha: float=None, initial_value: float = None):\n    \"\"\"Runs a Strategy over a Bootstrap/Monte Carlo of rewards.\n\n    Inputs:\n    -------\n    epsilon: float\n        The probability that the user will try a different machine at random in a given game.\n    alpha: float\n        The step size parameter used for exponential weighted averaging. A value of None\n        means that the mean is used.\n    initial_value: float\n        The prize expected by the player for each machine at the beginning of the game.\n    \"\"\"\n    # Initialize vectors\n    running_value_estimate = np.zeros((n_iterations, n_turns, n_machines))\n    running_count = np.zeros((n_iterations, n_turns, n_machines), dtype=int)\n    strategy_rewards = np.zeros((n_iterations, n_turns))\n    running_selection = np.zeros((n_iterations, n_turns), dtype=int)\n    selection = np.zeros(n_iterations, dtype=int)[:, None]\n\n    # Instantiate all random variables up front\n    random_selection = rng.integers(low=0, high=n_machines, size=(n_iterations, n_turns))\n    random_explore = rng.uniform(0, 1, size=(n_iterations, n_turns))\n    for i in range(n_turns):\n        if i < n_machines:\n            # Try all machines once.\n            selection = np.array([i]*n_iterations)[:, None]\n        else:\n            # Explore with some probability epsilon\n            explore = random_explore[:, i] < epsilon\n            selection[explore] = random_selection[explore, i][:, None]\n            # Otherwise, use greedy selection (select machine thought most valuable)\n            selection[~explore] = np.argmax(running_value_estimate[~explore, i-1, :], axis=1)[:, None]\n\n        running_selection[:, i] = selection[:, 0]\n\n        strategy_rewards[:, i] = np.take_along_axis(rewards[:, i, :], selection, axis=1)[:, 0]\n\n        if i > 0:\n            running_count[:, i, :] = running_count[:, i - 1, :]\n        update_count = np.zeros((n_iterations, n_machines))\n        np.put_along_axis(update_count, selection, 1, axis = 1)\n        running_count[:, i, :] = running_count[:, i, :] + update_count\n\n        if i < n_machines and initial_value is None:\n            # If initial_value is None, start with initial value observed in machines.\n            # NOTE: initial iterations could be randomized, but iterating along machines\n            # 1, 2, 3, ... is random enough for this exercise.\n            np.put_along_axis(running_value_estimate[:, i, :], selection, strategy_rewards[:, i][:, None], axis=1)\n        else:\n            if i == 0 and initial_value is not None:\n                # If there is an initial_value, start with that.\n                running_value_estimate[:, i, :] = initial_value\n            else:\n                running_value_estimate[:, i, :] = running_value_estimate[:, i - 1, :]\n\n            if alpha is not None:\n                # Exponential Weight Decay\n                step_size = alpha\n            else:\n                # Incremental Mean Update\n                step_size = 1/np.take_along_axis(running_count[:, i, :], selection, axis=1) \n            \n            update_flat = (\n                step_size\n                * (\n                    strategy_rewards[:, i][:, None] \n                    - np.take_along_axis(running_value_estimate[:, i, :], selection, axis=1))\n            )\n            update = np.zeros((n_iterations, n_machines))\n            np.put_along_axis(update, selection, update_flat, axis=1)\n            running_value_estimate[:, i, :] = running_value_estimate[:, i, :] + update\n\n    return running_value_estimate, running_count, strategy_rewards, running_selection\n\nexploitation_results = strategy(epsilon=0.0)\naverage_exploitation_rewards = np.mean(exploitation_results[2], axis=0)\n# Plotting\nsns.lineplot(x=turns, y=average_exploitation_rewards, color=palette[6], label=\"exploitation\")\nsns.lineplot(x=turns, y=average_oracle_rewards, color=palette[4], label=\"oracle\")\nsns.lineplot(x=turns, y=average_random_rewards, color=palette[5], label=\"random\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0.9, 1.2)\nplt.xlabel(\"turn\")\nplt.ylabel(\"average_prize\")\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Running Average of Prizes won by the Exploitation Strategy over 5000 iterations](index_files/figure-html/fig-average_reward_exploitation-output-1.png){#fig-average_reward_exploitation width=688 height=494 fig-alt='Line plot of the running Average of prizes won by the Exploitation Strategy over 5000 iterations.'}\n:::\n:::\n\n\nThis turns out to be a bad strategy, no better than random chance (in expectation) in our example. Remember, the actual prizes in this game are randomly generated from a distribution around the expected value. The player might be more likely to see a better prize from the better machine, but because they did not play the other machines again, they never found out if one of the others might have been better after all.\n\n### Exploration\n\nIn general, we will still want to exploit the information that we have found out about the machines in the game. We know that picking random machines on every turn is a strategy that will lose in the long term. But, we still need to check the other machines from time to see if we've maybe underestimated them. This leads us to the idea of exploration. With some probability, instead of picking the machine we think is best, we will instead pick one completely at random. \n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nexploratation_results = strategy(epsilon=0.2)\naverage_exploratation_rewards = np.mean(exploratation_results[2], axis=0)\n# Plotting\nsns.lineplot(x=turns, y=average_exploitation_rewards, color=palette[6], label=\"exploitation\", alpha = 0.3)\nsns.lineplot(x=turns, y=average_exploratation_rewards, color=palette[6], label=\"exploratation\")\nsns.lineplot(x=turns, y=average_oracle_rewards, color=palette[4], label=\"oracle\")\nsns.lineplot(x=turns, y=average_random_rewards, color=palette[5], label=\"random\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0.9, 1.2)\nplt.xlabel(\"turn\")\nplt.ylabel(\"average_prize\")\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Running Average of Prizes won by the Exploration Strategy over 5000 iterations](index_files/figure-html/fig-average_reward_exploration-output-1.png){#fig-average_reward_exploration width=688 height=494 fig-alt='Line plot of the running Average of prizes won by the Exploration Strategy over 5000 iterations.'}\n:::\n:::\n\n\nOkay! Now we're starting to see some progress! We've hit on a way to finally start making money on these prizes (in expectation at least). **But**, there's something weird going on, our success gets worse as the game goes on.\n\nWhat's happening here is a result of the fact that the expected prize given out by the machine is not constant over time. In fact, over a long enough time frame, I've set the machines to vary around our expected prize value of $0.99. Because our rule takes the average over all turns of the machine, when we are on turn 1000, we are still weighing the prize we got on turn 1 the same as the prize we got on turn 999.\n\n### Recency Bias\n\nIn order to combat this, what we want to do (in this particular situation) is to place more weight on recent observations. This will allow us to ride the wave when one machine is outperforming the others. We will do this using exponentially weighted averaging, with a parameter that weights recent observations just a little bit more highly than the average of all the observations we've seen so far. In this way, the ability of the early wins to influence our later decision making is reduced further and further with every turn.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nexp_weighting_results = strategy(epsilon=0.25, alpha=0.15)\naverage_exp_weighting_rewards = np.mean(exp_weighting_results[2], axis=0)\n# Plotting\nsns.lineplot(x=turns, y=average_exploitation_rewards, color=palette[6], label=\"exploitation\", alpha=0.3)\nsns.lineplot(x=turns, y=average_exploratation_rewards, color=palette[6], label=\"exploratation\", alpha=0.3)\nsns.lineplot(x=turns, y=average_exp_weighting_rewards, color=palette[6], label=\"exp_weighting\")\nsns.lineplot(x=turns, y=average_oracle_rewards, color=palette[4], label=\"oracle\")\nsns.lineplot(x=turns, y=average_random_rewards, color=palette[5], label=\"random\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0.9, 1.2)\nplt.xlabel(\"turn\")\nplt.ylabel(\"average_prize\")\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Running Average of Prizes won by the Exploration Strategy with exponentially weighted averaging over 5000 iterations](index_files/figure-html/fig-average_reward_exp_weighting-output-1.png){#fig-average_reward_exp_weighting width=688 height=494 fig-alt='Line plot of the running Average of prizes won by the Exploration Strategy with exponentially weighted averaging over 5000 iterations.'}\n:::\n:::\n\n\nBetter again! But, we have a period at the start of the game where it takes us about 50 turns to \"get up to speed\". Let's see if we can do something about that.\n\n### Optimistic Starting Conditions\n\nTo try and improve performance in the early turns, we will start be slightly optimistically assuming thae each machine will hand out an expected $1.10 on each round. this seems to work, but admittedly, it's not as impressive as the other strategies.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\noptimistic_results = strategy(epsilon=0.25, alpha=0.15, initial_value=1.1)\naverage_optimistic_rewards = np.mean(optimistic_results[2], axis=0)\n# Plotting\nsns.lineplot(x=turns, y=average_exploitation_rewards, color=palette[6], label=\"exploitation\", alpha=0.3)\nsns.lineplot(x=turns, y=average_exploratation_rewards, color=palette[6], label=\"exploratation\", alpha=0.3)\nsns.lineplot(x=turns, y=average_exp_weighting_rewards, color=palette[6], label=\"exp_weighting\", alpha=0.3)\nsns.lineplot(x=turns, y=average_optimistic_rewards, color=palette[6], label=\"optimistic\")\nsns.lineplot(x=turns, y=average_oracle_rewards, color=palette[4], label=\"oracle\")\nsns.lineplot(x=turns, y=average_random_rewards, color=palette[5], label=\"random\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0.9, 1.2)\nplt.xlabel(\"turn\")\nplt.ylabel(\"average_prize\")\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Running Average of Prizes won by the Exploration Strategy with exponentially weighted averaging and optimistic initial expectations over 5000 iterations](index_files/figure-html/fig-average_reward_exp_weighting_optimistic-output-1.png){#fig-average_reward_exp_weighting_optimistic width=688 height=494 fig-alt='Line plot of the running Average of prizes won by the Exploration Strategy with exponentially weighted averaging and optimistic initial expectations over 5000 iterations.'}\n:::\n:::\n\n\n## A Note on Risk\n\nSo, above we look at the average return to each strategy. But the average is only one part of the story. To see how we might end up, we will model the paths of 5000 games, each through 500 turns. We can then easily create a plot to see how players are likely to finish up if they start with $100 and follow each of these strategies. The charts below are path dependent, so if the player runs out of money, they're out of the game.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\ndef final_stats(rewards: np.array, initial_holding: float=100, cost_per_play: float=1.0):\n    holdings = np.cumsum(rewards - np.ones(rewards.shape[1])[None], axis=1) + initial_holding \n    final_holdings = holdings[:, -1]\n    final_holdings[np.any(holdings<=0, axis=1)] = 0\n\n    sorted_final_holdings = np.sort(final_holdings)\n    cdf_prob = np.arange(1, len(sorted_final_holdings) + 1) / len(sorted_final_holdings)\n    return sorted_final_holdings, cdf_prob\n\nstats_oracle = final_stats(oracle_rewards)\nstats_random = final_stats(random_rewards)\nstats_optimistic = final_stats(optimistic_results[2])\n\nplt.axvline(x=100, color=palette[0])\nsns.lineplot(x=stats_oracle[0], y=stats_oracle[1], color=palette[4], label=\"oracle\")\nsns.lineplot(x=stats_random[0], y=stats_random[1], color=palette[5], label=\"random\")\nsns.lineplot(x=stats_optimistic[0], y=stats_optimistic[1], color=palette[6], label=\"optimistic\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0, 1)\nplt.xlabel(\"final_prize_money\")\nplt.ylabel(\"continuous_density_function\")\n\nplt.annotate(\"breakeven\", (100, 0.1), (300, 0.18),\n    arrowprops=dict(facecolor=palette[0], edgecolor=palette[0],\n        arrowstyle=\"simple,tail_width=0.07,head_width=0.7,head_length=1\"))\nformatter_perc = ticker.FuncFormatter(lambda x, pos: f'{x:.0%}')\nformatter_dollar = ticker.FuncFormatter(lambda x, pos: f'${x:.0f}')\nplt.gca().yaxis.set_major_formatter(formatter_perc)\nplt.gca().xaxis.set_major_formatter(formatter_dollar)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Cumulative Density Function for 5000 iterations of Exploration Strategy with exponentially weighted averaging and optimistic initial expectations](index_files/figure-html/fig-cdf_prizes-output-1.png){#fig-cdf_prizes width=697 height=494 fig-alt='Cumulative Density Function for 5000 iterations of Exploration Strategy with exponentially weighted averaging and optimistic initial expectations.'}\n:::\n:::\n\n\nThe plots above tell us that we would likely have lost money over 55% of the time if we randomly pick a machine to play each time. Following the strategy we came up with with exploitation and exploration, using exponential weighting, and optimistic initial expectations we would have lost money less than 30% of the time. Due to randomness, even the oracle strategy would have resulted in losses about 5% of the time.\n\n## Summary\n\nHopefully this was at least a little interesting and provided some useful insights into decision making. To recap, we looked a specific type of decision: a decision we make repeatedly, where we only see the reward we get for the choices we make. Under these conditions, we can use the concepts of exploration, a bias towards more recent observations, and an initially optimistic view of expectations. Operating under uncertainty, where you never get to know what the oracle strategy might be, concepts like these help us to maximize our return.\n\n## Further Reading\n\nThere is a lot more to explore in reinforcement learning. First of all, there is the concept of a \"state\". This allows us to incorporate more information into a decision, e.g. \"I'd like Mexican food tonight\" would make a difference in selecting a restaurant. There are also methods in machine learning to allow for planning, so you can make more complicated decisions, e.g. deciding which move to make next in a game of chess, and methods that allow us to learn by example, e.g. by watching someone else play chess.\n\nFor a more detailed exploration of the topic, I highly recommend the foundational Reinforcement Learning: An Introduction by Sutton and Barto.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}