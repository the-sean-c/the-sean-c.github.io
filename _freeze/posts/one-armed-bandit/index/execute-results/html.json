{
  "hash": "5737f1f1974a535dedb4bd78a1f61792",
  "result": {
    "markdown": "---\ntitle: \"Decision-Making - Beating the Bandits\"\nauthor: \"Harlow Malloc\"\ndate: \"2023-08-03\"\ncategories: []\nimage: \"one-armed-bandits.png\"\ndraft: true\ntoc: true\nformat:\n    html:\n        code-fold: true\n---\n\n\n![](man-machines.jpeg){fig-align=\"center\"}\n\nDecisions are hard. We can't see how things will turn out, and we never have all the information we need to make the best decision right now. However, there are systematic ways to work through the choices we make to give us a better chance of coming out on top.\n\nOne way to view things is through the lens of reinforcement learning. This is a framework for decision making that assumes there is some environment out there to act on, and whatever we decide to do the environment gives us some feedback in the form of a reward or a punishment. Through these interactions, we can learn a better decision making rule.\n\nThere is a little more to the framework, such as the idea of a \"state\" that can impact the results of our actions. But, we will ignore that for now for the classic model I will talk about below: the multi-armed bandit. This model has the following features:\n\n- We are in a casino with slot machines (one-armed bandits[^1]).\n- There are many machines to choose from.\n- Expected reward can change over time.\n- The reward on a given play is randomly scattered around the expected value.\n\n    [^1]: The name comes from the fact that older mechanical slot machines had an arm on the side to make the machine work, and over the long run they will take all your money.\n\nThis type of decision can be seen in many real world situations, e.g. picking a restaurant, elements of a marketing campaign, or a vendor. This type of decision is also nice because it's relatively simple and it demonstrates some really nice elements of a decision-making strategy, which are the main takeaways from this blog:\n\n> - **Exploration v. Exploitation**: a really key concept to grasp. Do you explore new restaurants every weekend like an epicurean nomad, or do you decide you've found one that's good enough (exploiting what you already know)? Spoiler: you try to balance both.\n> - **Bias Towards Recency**: Things change all the time - you should change with them.\n> - **Optimistic Initial Expectations**: Believe it or not, until you know any better you're better off assuming the best.\n\n## Multi-Armed Bandits\n\nUsing a gambling machine allows us to bring this model squarely into the realm of probabilities, and it adds a natural system of rewards, i.e. prizes. So say, for example, you walk into a casino and start playing 3 machines. You go from one to the other over the course of 6 turns and get the following prizes. \n\nWhat do you do next?\n\n::: {#tbl-6_turns .cell tbl-cap='Results from 6 Turns' execution_count=2}\n``` {.python .cell-code}\ndf = pd.DataFrame(\n    {\n        \"turn\": [1, 2, 3, 4, 5, 6],\n        \"machine_id\": [1, 2, 3, 1, 2, 3],\n        \"prize\": [0.10, 1.32, 0.29, 1.18, 1.10, 0.17]\n    }\n)\n\n# Format table\nd = dict(selector=\"th\",\n    props=[('text-align', 'center')])\n\n(df.style.hide()\n    .format({'prize': '${:,.2f}'})\n    .set_properties(**{'width':'10em', 'text-align':'center'})\n    .set_table_styles([d]))\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```{=html}\n<style type=\"text/css\">\n#T_61a53 th {\n  text-align: center;\n}\n#T_61a53_row0_col0, #T_61a53_row0_col1, #T_61a53_row0_col2, #T_61a53_row1_col0, #T_61a53_row1_col1, #T_61a53_row1_col2, #T_61a53_row2_col0, #T_61a53_row2_col1, #T_61a53_row2_col2, #T_61a53_row3_col0, #T_61a53_row3_col1, #T_61a53_row3_col2, #T_61a53_row4_col0, #T_61a53_row4_col1, #T_61a53_row4_col2, #T_61a53_row5_col0, #T_61a53_row5_col1, #T_61a53_row5_col2 {\n  width: 10em;\n  text-align: center;\n}\n</style>\n<table id=\"T_61a53\">\n  <thead>\n    <tr>\n      <th id=\"T_61a53_level0_col0\" class=\"col_heading level0 col0\" >turn</th>\n      <th id=\"T_61a53_level0_col1\" class=\"col_heading level0 col1\" >machine_id</th>\n      <th id=\"T_61a53_level0_col2\" class=\"col_heading level0 col2\" >prize</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td id=\"T_61a53_row0_col0\" class=\"data row0 col0\" >1</td>\n      <td id=\"T_61a53_row0_col1\" class=\"data row0 col1\" >1</td>\n      <td id=\"T_61a53_row0_col2\" class=\"data row0 col2\" >$0.10</td>\n    </tr>\n    <tr>\n      <td id=\"T_61a53_row1_col0\" class=\"data row1 col0\" >2</td>\n      <td id=\"T_61a53_row1_col1\" class=\"data row1 col1\" >2</td>\n      <td id=\"T_61a53_row1_col2\" class=\"data row1 col2\" >$1.32</td>\n    </tr>\n    <tr>\n      <td id=\"T_61a53_row2_col0\" class=\"data row2 col0\" >3</td>\n      <td id=\"T_61a53_row2_col1\" class=\"data row2 col1\" >3</td>\n      <td id=\"T_61a53_row2_col2\" class=\"data row2 col2\" >$0.29</td>\n    </tr>\n    <tr>\n      <td id=\"T_61a53_row3_col0\" class=\"data row3 col0\" >4</td>\n      <td id=\"T_61a53_row3_col1\" class=\"data row3 col1\" >1</td>\n      <td id=\"T_61a53_row3_col2\" class=\"data row3 col2\" >$1.18</td>\n    </tr>\n    <tr>\n      <td id=\"T_61a53_row4_col0\" class=\"data row4 col0\" >5</td>\n      <td id=\"T_61a53_row4_col1\" class=\"data row4 col1\" >2</td>\n      <td id=\"T_61a53_row4_col2\" class=\"data row4 col2\" >$1.10</td>\n    </tr>\n    <tr>\n      <td id=\"T_61a53_row5_col0\" class=\"data row5 col0\" >6</td>\n      <td id=\"T_61a53_row5_col1\" class=\"data row5 col1\" >3</td>\n      <td id=\"T_61a53_row5_col2\" class=\"data row5 col2\" >$0.17</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nI will be modelling the prizes won by a player as an exponential distribution. This seems appropriate because it will give us many small wins and a few very big wins. Below I have an example plot of the exponential distribution, with some sample rewards. As you can see, there are far more rewards below the mean than above it, but the big rewards are much bigger. The model will be set up with a cost to play each game of $1, and an expected reward of $0.95. The player will start with $100.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Parameters\nrng = np.random.default_rng(7)\nmean = 0.99\nlambda_param = 1.0 / mean\nx = np.linspace(0, 6, 400)  # Generate x values\ny = lambda_param * np.exp(-lambda_param * x)  # Exponential distribution function\n\n# Sample 50 points and jitter for dodging\nsample_points_x = rng.exponential(mean, 50)\njitter = 0.05  # Adjust this value for more/less jitter\nsample_points_y = [rng.uniform(-jitter, jitter) for _ in range(50)]\n\npalette = sns.color_palette()\n\nsns.lineplot(x=x, y=y)\nplt.plot([mean, mean], [lambda_param * np.exp(-lambda_param * mean), 0], color=palette[0])\nplt.annotate(\"expected mean\", (mean, 0.2), (1.5, 0.5),\n    arrowprops=dict(facecolor=palette[0], edgecolor=palette[0],\n        arrowstyle=\"simple,tail_width=0.07,head_width=0.7,head_length=1\"))\nplt.annotate(\"random samples\", (2.5, 0), (4, 0.3),\n    arrowprops=dict(facecolor=palette[0], edgecolor=palette[0],\n        arrowstyle=\"simple,tail_width=0.07,head_width=0.7,head_length=1\"))\nsns.scatterplot(x=sample_points_x, y=sample_points_y)\n# plt.grid(axis='y', color='grey', linestyle='--', linewidth=0.5)\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.0f}')\nplt.gca().xaxis.set_major_formatter(formatter)\nplt.xlabel('prize')\nplt.ylabel('probability density function')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Example of an Exponential Distribution](index_files/figure-html/fig-exponential_distribution_pdf-output-1.png){#fig-exponential_distribution_pdf width=589 height=429 fig-alt='A plot of the exponential distribution probability density overlain by some sample points.'}\n:::\n:::\n\n\nMachines with an identical distribution of random prizes are not interesting though. Why? Well, there's no \"good\" machine to pick. Whether you pick one machine or hop around, you would have the same expected prize. The actual prize you win will vary through random chance which is not predictable.\n\nInstead, we are interested in the cases where playing different machines has different expected outcomes. In this case, we want to have a systematic approach to deciding which machine to play to maximize our rewards. This maps back to other decisions we might make in our day-to-day lives, such as choosing a restaurant. The meals you get at a restaurant would probably have a less variability than my model here, but despite on-days and off-days, some restaurants are just better on average.\n\nCreating a random walk with mean recursion for each of the three machines looks like this (noting that these are *expected* prizes, not actual prizes):\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nn_iterations = 500\nn_machines = 3\nn_turns = 1000\n\nsigma = 0.2\ncost_per_game = 1\nexpected_prize = 0.99\nstarting_value = 100\n\nrng = np.random.default_rng(5)\n\nmean_lin = np.ones(n_machines) * expected_prize\nvar_lin = np.ones(n_machines) * (sigma**2)\n\nmean_log = np.log(mean_lin**2 / (np.sqrt(mean_lin**2 + var_lin)))\nvar_log = np.log(1 + var_lin / mean_lin**2) \ncov_log = np.diag(var_log)\n\n# Mean-reverting process\nnoise_log = rng.multivariate_normal(np.zeros(n_machines), cov_log, size=(n_iterations, n_turns + 200))\n\ne_rewards_log = np.ones_like(noise_log) * mean_log\ntheta = 0.01\nfor i in range(1, n_turns + 200):\n    e_rewards_log[:, i, :] = e_rewards_log[:, i - 1, :] + (\n        0.01 * (mean_log - e_rewards_log[:, i - 1, :]) + 0.15 * noise_log[:, i, :]\n        )\ne_rewards = np.exp(e_rewards_log[:, 200:])\n\n# Generate rewards from expected rewards\nrewards = rng.exponential(e_rewards)\n\nmachine_labels = [str(i + 1) for i in range(rewards.shape[2])]\n\nrewards_df = pd.DataFrame(rewards[0, :, :])\nrewards_df.columns = machine_labels\nrewards_df[\"turn\"] = list(range(1, rewards.shape[1] + 1))\nrewards_df = rewards_df.melt(\n    id_vars=\"turn\",\n    value_vars=machine_labels,\n    value_name=\"prize\",\n    var_name=\"machine\",)\n\ne_rewards_df = pd.DataFrame(e_rewards[0, ::])\ne_rewards_df.columns = machine_labels\ne_rewards_df[\"turn\"] = list(range(1, e_rewards.shape[1] + 1))\ne_rewards_df = e_rewards_df.melt(\n    id_vars=\"turn\",\n    value_vars=machine_labels,\n    value_name=\"expected_prize\",\n    var_name=\"machine\",)\n\nrewards_df = rewards_df.merge(e_rewards_df, on=(\"turn\", \"machine\"), how=\"left\")\n\n# Plot\nax = sns.lineplot(data=rewards_df, x=\"turn\", y=\"expected_prize\", hue=\"machine\")\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.ylim(0.6, 1.8)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Expected Rewards for each of 3 Machines](index_files/figure-html/fig-expected_reward-output-1.png){#fig-expected_reward width=669 height=434 fig-alt='A line-plot of the expected rewards to each of 3 machines.'}\n:::\n:::\n\n\nThe actual prizes you would win if you played all the machines at the same time would look as follows:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nax = sns.scatterplot(data=rewards_df, x=\"turn\", y=\"prize\", hue=\"machine\", s=7)\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.ylim(0.0, 11)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Actual Rewards for each of 3 Machines](index_files/figure-html/fig-reward-output-1.png){#fig-reward width=677 height=429 fig-alt='A scatter plot of the rewards to each of 3 machines.'}\n:::\n:::\n\n\n### The Oracle Strategy\n\nWe can also plot what we call the \"oracle\", which is a model with perfect information, i.e. it knows which machine is the best to play on each turn. The expected return to the oracle looks like this:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\noracle_e_rewards = np.max(e_rewards[0], axis=1)\nturns = list(range(1, len(oracle_e_rewards)+1))\n\nax = sns.lineplot(data=rewards_df, x=\"turn\", y=\"expected_prize\", hue=\"machine\", alpha=0.3)\nsns.lineplot(x=turns, y=oracle_e_rewards, color=palette[4], label=\"oracle\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 1, 1])\nplt.ylim(0.6, 1.8)\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Expected Prizes under the Oracle Strategy](index_files/figure-html/fig-expected_reward_oracle-output-1.png){#fig-expected_reward_oracle width=679 height=475 fig-alt='A scatter plot of the prizes won by the Oracle Strategy.'}\n:::\n:::\n\n\nWe can track how successful the oracle strategy was over time by taking the running average of the prizes it won throughout the game:\n\nThat's not astonishing - even a strategy with perfect information is only barely coming out ahead - remember that each game costs $1 to play!\n\n## The Random Strategy\n\nAnother strategy we could use is to pick a machine to play at random in each round. \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Oracle Strategy\noracle_choice = np.argmax(e_rewards, axis=2, keepdims=True)\noracle_rewards = np.take_along_axis(rewards, oracle_choice, axis=2)\noracle_rewards = np.squeeze(oracle_rewards)\n\nturns = np.array(list(range(1, n_turns+1)))\naverage_oracle_rewards = np.cumsum(oracle_rewards, axis=1) / turns\naverage_oracle_rewards = np.mean(average_oracle_rewards, axis=0)\n\n# Random Strategy\nrandom_choice = rng.integers(0, n_machines, (n_iterations, n_turns))\nrandom_choice = np.expand_dims(random_choice, 2)\nrandom_rewards = np.take_along_axis(rewards, random_choice, axis=2)\nrandom_rewards = np.squeeze(random_rewards)\n\nturns = np.array(list(range(1, random_rewards.shape[1]+1)))\naverage_random_rewards = np.cumsum(random_rewards, axis=1) / turns\naverage_random_rewards = np.mean(average_random_rewards, axis=0)\n\n# Plotting\nsns.lineplot(x=turns, y=average_oracle_rewards, color=palette[4], label=\"oracle\")\nsns.lineplot(x=turns, y=average_random_rewards, color=palette[5], label=\"random\")\n\nplt.grid(axis='y', color='#E5E5E5')\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)\nplt.tight_layout(rect=[0, 0, 0.98, 1])\nplt.ylim(0.6, 1.8)\nplt.xlabel(\"turn\")\nplt.ylabel(\"average_prize\")\n\nformatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Running Average of Prizes won by Random Strategy ](index_files/figure-html/fig-average_reward_random-output-1.png){#fig-average_reward_random width=675 height=494 fig-alt='Running Average of Prizes won by Random Strategy.'}\n:::\n:::\n\n\n## Exploration v. Exploitation\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\ndef strategy(epsilon: float = 0.05, alpha: float=None, initial_value: float = None):\n    # Initialize vectors\n    running_value_estimate = np.zeros((n_iterations, n_turns, n_machines))\n    running_count = np.zeros((n_iterations, n_turns, n_machines), dtype=int)\n    exploit_rewards = np.zeros((n_iterations, n_turns))\n    running_selection = np.zeros((n_iterations, n_turns), dtype=int)\n    selection = np.zeros(n_iterations, dtype=int)[:, None]\n\n    # Instantiate all random variables up front\n    random_selection = rng.integers(low=0, high=n_machines, size=(n_iterations, n_turns))\n    random_explore = rng.uniform(0, 1, size=(n_iterations, n_turns))\n    for i in range(n_turns):\n        if i < n_machines:\n            # Try all machines once.\n            selection = np.array([i]*n_iterations)[:, None]\n        else:\n            # Explore with some probability epsilon\n            explore = random_explore[:, i] < epsilon\n            selection[explore] = random_selection[explore, i][:, None]\n            # Otherwise, use greedy selection (select machine thought most valuable)\n            selection[~explore] = np.argmax(running_value_estimate[~explore, i-1, :], axis=1)[:, None]\n\n        running_selection[:, i] = selection[:, 0]\n\n        exploit_rewards[:, i] = np.take_along_axis(rewards[:, i, :], selection, axis=1)[:, 0]\n\n        if i > 0:\n            running_count[:, i, :] = running_count[:, i - 1, :]\n        update_count = np.zeros((n_iterations, n_machines))\n        np.put_along_axis(update_count, selection, 1, axis = 1)\n        running_count[:, i, :] = running_count[:, i, :] + update_count\n\n        if i < n_machines and initial_value is None:\n            # If initial_value is None, start with initial value observed in machines.\n            # NOTE: initial iterations could be randomized, but iterating along machines\n            # 1, 2, 3, ... is random enough for this exercise.\n            np.put_along_axis(running_value_estimate[:, i, :], selection, exploit_rewards[:, i][:, None], axis=1)\n        else:\n            if i == 0 and initial_value is not None:\n                # If there is an initial_value, start with that.\n                running_value_estimate[:, i, :] = initial_value\n            else:\n                running_value_estimate[:, i, :] = running_value_estimate[:, i - 1, :]\n\n            if alpha is not None:\n                # Exponential Weight Decay\n                step_size = alpha\n            else:\n                # Incremental Mean Update\n                step_size = 1/np.take_along_axis(running_count[:, i, :], selection, axis=1) \n            \n            update_flat = (\n                step_size\n                * (\n                    exploit_rewards[:, i][:, None] \n                    - np.take_along_axis(running_value_estimate[:, i, :], selection, axis=1))\n            )\n            update = np.zeros((n_iterations, n_machines))\n            np.put_along_axis(update, selection, update_flat, axis=1)\n            running_value_estimate[:, i, :] = running_value_estimate[:, i, :] + update\n\n    return running_value_estimate, running_count, exploit_rewards, running_selection\n\nstrategy()\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\n(array([[[0.99711511, 0.        , 0.        ],\n         [0.        , 0.27263818, 0.        ],\n         [0.        , 0.        , 0.48611999],\n         ...,\n         [0.97563471, 1.14760946, 1.1166643 ],\n         [0.97563471, 1.15160298, 1.1166643 ],\n         [0.97563471, 1.14814528, 1.1166643 ]],\n \n        [[1.13475557, 0.        , 0.        ],\n         [0.        , 1.19279756, 0.        ],\n         [0.        , 0.        , 2.50067941],\n         ...,\n         [0.88020979, 0.85264229, 1.08628566],\n         [0.88020979, 0.85264229, 1.08685859],\n         [0.88020979, 0.85264229, 1.08607874]],\n \n        [[0.66958913, 0.        , 0.        ],\n         [0.        , 0.40909172, 0.        ],\n         [0.        , 0.        , 0.55980557],\n         ...,\n         [1.006638  , 0.87517686, 1.02381959],\n         [1.006638  , 0.87517686, 1.02331022],\n         [1.006638  , 0.87517686, 1.0227806 ]],\n \n        ...,\n \n        [[0.21011174, 0.        , 0.        ],\n         [0.        , 0.09810636, 0.        ],\n         [0.        , 0.        , 2.00004522],\n         ...,\n         [0.94856059, 0.85881829, 0.85751681],\n         [0.94882643, 0.85881829, 0.85751681],\n         [0.94937623, 0.85881829, 0.85751681]],\n \n        [[1.3111557 , 0.        , 0.        ],\n         [0.        , 1.82164069, 0.        ],\n         [0.        , 0.        , 0.02929256],\n         ...,\n         [1.30090478, 1.0015106 , 1.00642403],\n         [1.2989968 , 1.0015106 , 1.00642403],\n         [1.29810696, 1.0015106 , 1.00642403]],\n \n        [[0.00917046, 0.        , 0.        ],\n         [0.        , 1.87637832, 0.        ],\n         [0.        , 0.        , 1.60088245],\n         ...,\n         [0.72962202, 0.62721164, 0.90539239],\n         [0.72962202, 0.62721164, 0.90545992],\n         [0.72962202, 0.62721164, 0.90566379]]]),\n array([[[  1,   0,   0],\n         [  1,   1,   0],\n         [  1,   1,   1],\n         ...,\n         [ 24, 300, 674],\n         [ 24, 301, 674],\n         [ 24, 302, 674]],\n \n        [[  1,   0,   0],\n         [  1,   1,   0],\n         [  1,   1,   1],\n         ...,\n         [ 23,  23, 952],\n         [ 23,  23, 953],\n         [ 23,  23, 954]],\n \n        [[  1,   0,   0],\n         [  1,   1,   0],\n         [  1,   1,   1],\n         ...,\n         [ 31,  21, 946],\n         [ 31,  21, 947],\n         [ 31,  21, 948]],\n \n        ...,\n \n        [[  1,   0,   0],\n         [  1,   1,   0],\n         [  1,   1,   1],\n         ...,\n         [841,  42, 115],\n         [842,  42, 115],\n         [843,  42, 115]],\n \n        [[  1,   0,   0],\n         [  1,   1,   0],\n         [  1,   1,   1],\n         ...,\n         [617,  26, 355],\n         [618,  26, 355],\n         [619,  26, 355]],\n \n        [[  1,   0,   0],\n         [  1,   1,   0],\n         [  1,   1,   1],\n         ...,\n         [ 13,  17, 968],\n         [ 13,  17, 969],\n         [ 13,  17, 970]]]),\n array([[0.99711511, 0.27263818, 0.48611999, ..., 0.14070568, 2.34965881,\n         0.10737673],\n        [1.13475557, 1.19279756, 2.50067941, ..., 0.54180787, 1.63228403,\n         0.34287904],\n        [0.66958913, 0.40909172, 0.55980557, ..., 0.69579096, 0.54144621,\n         0.52123163],\n        ...,\n        [0.21011174, 0.09810636, 2.00004522, ..., 0.79751063, 1.17240097,\n         1.41230267],\n        [1.3111557 , 1.82164069, 0.02929256, ..., 1.5080654 , 0.12177702,\n         0.74818562],\n        [0.00917046, 1.87637832, 1.60088245, ..., 0.18737384, 0.97083078,\n         1.1032136 ]]),\n array([[0, 1, 2, ..., 1, 1, 1],\n        [0, 1, 2, ..., 2, 2, 2],\n        [0, 1, 2, ..., 0, 2, 2],\n        ...,\n        [0, 1, 2, ..., 0, 0, 0],\n        [0, 1, 2, ..., 0, 0, 0],\n        [0, 1, 2, ..., 2, 2, 2]]))\n```\n:::\n:::\n\n\nFor a more detailed exploration of the topic, I highly recommend the foundational Reinforcement Learning: An Introduction by Sutton and Barto.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}