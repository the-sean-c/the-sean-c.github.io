{
  "hash": "e133b51f933573267dcc1407d84561f9",
  "result": {
    "markdown": "---\ntitle: \"Finding Markov\"\nsubtitle: \"Fitting Hidden Markov Models\" \nauthor: \"Sean Daly\"\ndate: \"2023-10-30\"\ncategories:\n    - simulation\n    - tail-risk\nimage: \"man-and-chart.jpeg\"\ntoc: true\nformat:\n    html:\n        code-fold: true\n---\n\n::: {#imports .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport seaborn as sns\nfrom IPython.display import HTML, Markdown, display\nfrom matplotlib.ticker import PercentFormatter, ScalarFormatter\nfrom tabulate import tabulate\n\npalette = sns.color_palette()\n```\n:::\n\n\n![](man-and-chart.jpeg){fig-align=\"center\"}\n\nSometimes one model of the world won't do. Often, we try to fit a well-known model to some observations we have made, and just doesn't fit. Here we will look at the particular situation where observations are generated by several distributions using a mechanism known as a Hidden Markov Model (HMM).\n\nBelow, we will look at what a Hidden Markov Model is, with a basic example. Then we will create a model to simulate the S&P500. The model does a fairly good job of picking out the major panics of the past 150 years. Interestingly, it indicates that the chance of slipping into a panic in any given month over the period was about 1%. Finally, we will use the model to create some simulations, showing that these models have some advantages in being able to incorporate the risk of a severely negative event that Gaussian models by themselves cannot.\n\n## Hidden Markov Models\n\nThe idea is that the system we are trying to model has several states, and the distribution of observations varies depending on the state that the system is in. You could imagine a call center with a helpful and an unhelpful representative, the results of reaching one will be better than the results of reaching the other.\n\nHowever, with *Hidden* Markov Models, it is not quite as easy as working with call center representatives, as we will not get to know who is on the other line. All we get are the results, and we have to try and piece together the details of which representative we got based on the outcomes we saw, and then try to model the distribution of outcomes for each of them.\n\n### Toy Example: Moody Dog\n\nSay we have a dog, and that this dog eats more food when it's happy. Let's say the dog eats the following quantities (both assumed to be normally distributed):\n\n- When happy, the dog eats an average of 250g, with a standard deviation of 50g.\n- When sad, the dog eats an average of 150g, with a standard deviation of 30g.\n\nThe above distributions are known as emission probability distributions, modelling the emitted observation (how much the dog eats), based on the internal state of the dog (its mood).\n\nWe will model the mood of our dog (i.e. the dog's state) as a Markov Process. By this, we just mean that the dog's mood one day is dependent only on it's mood the previous day. It does not matter if the dog was happy or sad on any of the days before that.\n\nOur Markov Process has discrete time as we step from day to day, and a discrete and countable number of states, i.e. happy and sad. We will model our dog's transition from day to day as follows:\n\n- Happy today:\n    - 70% chance of being happy tomorrow,\n    - 30% chance of being sad.\n- Sad today:\n    - 50% chance of being happy tomorrow,\n    - 50% chance of being sad.\n\nWe will write this as a matrix like so, where $P_{ij}$ means the probability of going from row $i$ to column $j$, arranged so that row and column `0` represent \"sad\", and row and column `1` represent \"happy\".\n\n$$\nP_{ij} =\n\\begin{bmatrix}\n0.5 & 0.5 \\\\\n0.3 & 0.7\n\\end{bmatrix}\n$$\n\nWe will assume that the initial probabilities are the steady state probabilities of the dog being either happy or sad, calculated as follows:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ntransition_matrix = np.array([\n    [0.5, 0.5],\n    [0.3, 0.7]\n])\n# Need left eigenvector\np = np.linalg.eig(transition_matrix.T).eigenvectors[:, 1]\n# Normalize so probabilities sum to one.\ninitial_probabilities = p / np.sum(p)\ninitial_probabilities\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\narray([0.375, 0.625])\n```\n:::\n:::\n\n\nThat means that in the long run, the dog is expected to be happy 62.5% of the time, and sad 37.5% of the time.\n\nKnowing all this we can then simulate a few days and see how much the dog eats:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nn_days = 25\nsimulated_data = []\nrng = np.random.default_rng(1)\nhappy = rng.binomial(n=1, p=0.625)\nmu = [150, 250]\nsigma = [30, 50]\nfor day in range(n_days):\n    food_eaten = rng.normal(mu[happy], sigma[happy])\n    simulated_data.append({\n        \"day\": day,\n        \"happy\": happy,\n        \"food_eaten\": food_eaten\n    })\n    happy = rng.binomial(n=1, p=transition_matrix[happy, 1])\n\nsimulated_data = pd.DataFrame(simulated_data).set_index(\"day\")\n\nfig, ax = plt.subplots()\nsns.lineplot(data=simulated_data, x=\"day\", y=\"food_eaten\", ax=ax)\n\ny1, y2 = ax.get_ylim()\nfor i, row in simulated_data.iterrows():\n    if row[\"happy\"]:\n        ax.fill_between(\n            [i-0.5, i + 0.5], \n            y1=y1, \n            y2=y2, \n            color='green',\n            edgecolor=None, \n            alpha=0.2,\n            label=\"happy\"\n            )\n\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Simulated Amounts of Food Eaten by Dog (g), Happy Days Shaded Green](index_files/figure-html/fig-dog_simulation-output-1.png){#fig-dog_simulation width=593 height=429 fig-alt='A line plot showing how much food a dog ate each day, with shading showing whether it was happy or not.'}\n:::\n:::\n\n\nAnd, we can see how the distributions are combined together:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nx = np.linspace(50, 400, 1000)\nfig, ax = plt.subplots()\nplt.plot(x, stats.norm.pdf(x, mu[0], sigma[0]), label=\"sad\")\nplt.plot(x, stats.norm.pdf(x, mu[1], sigma[1]), label=\"happy\")\nplt.plot(x, \n    0.375 * stats.norm.pdf(x, mu[0], sigma[0])\n        + 0.625 * stats.norm.pdf(x, mu[1], sigma[1]),\n    label=\"combined\")\n\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.tick_params(axis='x', which='both', bottom=True, left=True)\nplt.legend(frameon=False)\nax.set_yticks([])\nax.set_yticklabels([])\nplt.xlabel(\"food_eaten\")\nplt.ylabel(\"probability_density\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Probability Density Functions for Amounts Eaten by Dog (g) Based on Mood](index_files/figure-html/fig-dog_distribution-output-1.png){#fig-dog_distribution width=558 height=429 fig-alt='Probability Density Functions for amounts of food eaten by dog with happy, sad, and combined across many days.'}\n:::\n:::\n\n\nIn the figures above, we can see that there is some overlap between the amounts eaten when happy and when sad. Some days the dog will eat more when sad than it does on another day when happy. As good dog owners, we would know whether our dog is happy or not, but *in general, we will not know the internal state of the system we are trying to model*. \n\nThe unseen state is the \"hidden\" part of a Hidden Markov Model. There are some ways to estimate which state is generating the observations, and once we can estimate this, we can estimate parameters for the system and create simulations.\n\n## Something More Serious: Separating Stock Returns\n\nNow we will look at something where we don't know the underlying probability distributions: stock market returns. The reason these are interesting is that a plot of returns (daily, monthly etc.) looks like a Gaussian distribution, but big losses happen more frequently that would be expected under a Gaussian distribution of returns.\n\n### Data\n\nThe S&P 500 index price data was retrieved from Robert Shiller's Online Data website, and contains a reconstituted time series back to 1871.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndf = pd.read_pickle(\"posts/hidden-markov-model/data/shiller_data.pkl\")\ndata = (df[\"s&p_comp_p\"].pct_change())[1:].to_numpy()\ndf[\"s&p_perf\"] = np.insert(data, 0, np.nan)\n\nsns.lineplot(df[\"s&p_comp_p\"])\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.yscale('log')\nformatter = ScalarFormatter()\nformatter.set_scientific(False)\nplt.gca().yaxis.set_major_formatter(formatter)\nplt.ylabel(\"s&p500_index\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Price of the S&P 500 Index](index_files/figure-html/fig-sp500_price-output-1.png){#fig-sp500_price width=601 height=429 fig-alt='Line chart showing the price of the s&P 500 Index.'}\n:::\n:::\n\n\nThe performance numbers we are analysing are monthly returns in that dataset, which are distributed like this:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nsns.histplot(df[\"s&p_perf\"], edgecolor=None)\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.gca().xaxis.set_major_formatter(PercentFormatter(1))\nplt.ylabel(\"count\")\nplt.xlabel(\"s&p500_monthly_performance\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Distribution of the S&P 500 Index Monthly Performance](index_files/figure-html/fig-sp500_performance-output-1.png){#fig-sp500_performance width=593 height=429 fig-alt='Histogram of monthly performance values for the s&P 500 Index.'}\n:::\n:::\n\n\nThis shows that the distribution has a negative skew, with a visibly fat tail on the left side. The datapoint with a +50% performance would warrant a deeper review of the data if this analysis was being relied on in a meaningful way. It occurred in September 1932 in the dataset, but a google search did not provide a ready explanation.\n\n### Model Description\n\nIn the toy example above, we assumed that there were 2 states that the dog could be in, and each was associated with a different emission probability distribution. For stock returns, we will assume that there are 3 states:\n\n- Normal - This will be the case the majority of the time, characterized by a Gaussian distribution.\n- Relief - This will be a rare case, where large gains are made, characterized by a Pareto distribution. (I originally called this \"euphoria\", but as we see below it usually happens during a pull back after a panic).\n- Panic - Another rare case, were large losses are made, characterized by a Pareto distribution.\n\nMany people are familiar with the Gaussian Distribution, the classic \"bell curve\" with a mean and standard deviation. Fewer are aware of the Pareto distribution, though we might have heard of the 80/20 rule that inspired it. I'm using it here as it is suggested by Nassim Taleb in his Technical Incerto. It is a distribution with fat tails, with the following probability density function (pdf):\n\n$$\nP(X > x) = \\begin{cases}\n\\frac{\\alpha k^{\\alpha}}{x^{(\\alpha + 1)}} & \\text{for } x \\geq k, \\\\\n0 & \\text{for } x < k.\n\\end{cases}\n$$\n\nWhere:\n\n- $\\alpha > 0$ is the shape parameter, determining the thickness of the tails, where a lower value means thicker tails.\n- $k > 0$ is the scale parameter, setting the minimum value for the distribution.\n\nAn interesting interpretation of the scale parameter is as follows. Say the populations of our cities are Pareto-distributed with $\\alpha=2$. If there is a 10% probability that a city has a population greater than 10 million, then the probability that there are more than twice that number (i.e. more than 20 million) would be a $10\\% \\times (1/2)^\\alpha = 10\\% \\times (1/2)^2 = 2.5\\%$.  \n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nalpha = 3 \nk = 1\n\nx = np.linspace(k, 4, 100)\npdf = stats.pareto.pdf(x, alpha, loc=0, scale=k) \n\nfig, ax = plt.subplots()\nplt.plot(x, pdf, label=f'α={alpha}, k={k}')\n\nsns.despine(left=True, bottom=True, top=True, right=True)\nplt.legend(frameon=False)\nplt.xlabel(\"x\")\nplt.ylabel(\"probability_density\")\nax.set_yticks([])\nax.set_yticklabels([])\nplt.xlim([1, 4])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Probability Density Function for Generic Pareto Distribution](index_files/figure-html/fig-pareto_distribution-output-1.png){#fig-pareto_distribution width=569 height=429 fig-alt='Probability Density Function for Generic Pareto Distribution.'}\n:::\n:::\n\n\n### Model Fitting\n\nThe general approach to fitting the model will be Maximum Likelihood Estimation (MLE). However, we are fitting 2 models which interact with each other, not just one. So, to allow ourselves to do that we will use dynamic programming, following a general algorithm like this:\n\n1. Start with assumed transition probability distribution and emission probability distributions.\n2. Given the transition probability, find the optimal parameters of the emission probability distributions to fit the data through MLE.\n3. Given the updated emission probability distributions, find the optimal transition probabilities using the forward-backward algorithm.\n4. Repeat steps 2. and 3. until convergence.\n\nAs this is a numerical method, it can be prone to finding local optima, so in general it would be run several times with different initial assumptions. However, looking at the results, this wasn't necessary here.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nclass HMM:\n    def __init__(self):\n        # Probability of being in each state at t=0\n        self.initial = np.array([0.1, 0.8, 0.1])\n        # Probability of being in a final state, given that you are in that state\n        self.final = np.ones(3)\n        # Initial transition matrix, from column to row.\n        self.transitions = np.array(\n            [\n                [0.1, 0.8, 0.1],\n                [0.1, 0.8, 0.1],\n                [0.1, 0.8, 0.1],\n            ]\n        )\n        # Initial parameters for emission probability distributions\n        self.emissions = [\n            {\n                \"type\": stats.pareto,\n                \"params\": {\n                    \"b\": 1,\n                    \"loc\": 0.0,\n                    \"scale\": 0.1,\n                },\n                \"fit_params\": {\n                    \"floc\": 0.0, # Loc overlaps with the scale parameter, fix at 0\n                },\n                \"negative_emission\": True,\n            },\n            {\n                \"type\": stats.norm,\n                \"params\": {\n                    \"loc\": 0.007,\n                    \"scale\": 0.01,\n                },\n                \"fit_params\": {},\n                \"negative_emission\": False,\n            },\n            {\n                \"type\": stats.pareto,\n                \"params\": {\n                    \"b\": 1,\n                    \"loc\": 0.0,\n                    \"scale\": 0.1,\n                },\n                \"fit_params\": {\n                    \"floc\": 0.0, # Loc overlaps with the scale parameter, fix at 0\n                },\n                \"negative_emission\": False,\n            },\n        ]\n\n    def _pdf_emissions(self, Y: np.array):\n        \"\"\"Return value of pdf at each of the emissions.\"\"\"\n        pdfs = np.zeros(shape=(Y.shape[0], len(self.emissions)))\n        for i, distribution in enumerate(self.emissions):\n            # Required for positive-only Pareto Distribution\n            if distribution[\"negative_emission\"] == True:\n                _Y = -Y\n            else:\n                _Y = Y\n\n            pdfs[:, i] = distribution[\"type\"].pdf(x=_Y, **distribution[\"params\"])\n\n        return pdfs\n\n    def _forward_pass(self, Y: np.array, pdfs: np.array):\n        \"\"\"Return normalized probabilities of emission for each state given history.\"\"\"\n        alpha = np.zeros(shape=(Y.shape[0], len(self.emissions)))\n        alpha[0, :] = self.initial * pdfs[0, :]\n        norm = 1 / np.sum(alpha[0, :])\n        alpha[0, :] = alpha[0, :] * norm\n\n        for i in range(1, Y.shape[0]):\n            alpha[i, :] = (alpha[i - 1, :] @ self.transitions) * pdfs[i, :]\n            norm = 1 / np.sum(alpha[i, :])\n            alpha[i, :] = alpha[i, :] * norm\n        return alpha\n\n    def _backward_pass(self, Y: np.array, pdfs: np.array):\n        \"\"\"Return normalized probabilities of emission for each state given future.\"\"\"\n        beta = np.zeros(shape=(Y.shape[0], len(self.emissions)))\n        beta[-1, :] = self.final\n\n        for i in range(Y.shape[0] - 2, -1, -1):\n            beta[i, :] = (beta[i + 1, :] @ self.transitions.T) * pdfs[i + 1, :]\n            norm = 1 / np.sum(beta[i, :])\n            beta[i, :] = beta[i, :] * norm\n\n        return beta\n\n    def _update_state_transitions(self, Y: np.array):\n        pdfs = self._pdf_emissions(Y)\n        pdfs[pdfs == 0] = 1e-100\n\n        alpha = self._forward_pass(Y, pdfs=pdfs)\n        beta = self._backward_pass(Y, pdfs=pdfs)\n\n        gamma = alpha * beta\n        gamma = gamma / np.sum(gamma, axis=1)[:, None]\n\n        xi = (\n            alpha[:-1, :, None]\n            * self.transitions[None, :, :]\n            * beta[1:, None, :]\n            * pdfs[1:, None, :]\n        )\n        norm = 1 / np.sum(xi, axis=(1, 2))[:, None, None]\n        xi = xi * norm\n\n        self.initial = gamma[0, :]\n        # NOTE: added initial here so sum doesn't become zero.\n        self.transitions = (\n            np.sum(xi, axis=0, initial=1e-20)\n            / np.sum(gamma, axis=0, initial=1e-20)[:, None]\n        )\n        norm = 1 / np.sum(self.transitions, axis=1)[:, None]\n        self.transitions = self.transitions * norm\n        self.gamma = gamma\n        self.states = np.argmax(gamma, axis=1)\n\n    def fit(self, Y: np.array, n_iter=300):\n        for _ in range(n_iter):\n            self._update_state_transitions(Y)\n            for i, distribution in enumerate(self.emissions):\n                _Y = Y[self.states == i]\n                if distribution[\"negative_emission\"] == True:\n                    _Y = -_Y\n\n                if distribution[\"type\"] == stats.pareto:\n                    _Y = _Y[_Y > 0]\n                    if len(_Y) > 0:\n                        params = stats.pareto.fit(\n                            data=_Y,\n                            **distribution[\"fit_params\"],\n                        )\n                        distribution[\"params\"] = {\n                            \"b\": params[0],\n                            \"loc\": params[1],\n                            \"scale\": params[2],\n                        }\n                elif distribution[\"type\"] == stats.norm:\n                    if len(_Y) > 0:\n                        params = stats.norm.fit(\n                            data=_Y,\n                            **distribution[\"fit_params\"],\n                        )\n                        distribution[\"params\"] = {\n                            \"loc\": params[0],\n                            \"scale\": params[1],\n                        }\n\n    def simulate(self, n_periods, burn_in_periods=0, seed=None):\n        rng = np.random.default_rng(seed)\n        np.random.seed(seed) # Used by scipy stats\n\n        all_selections = []\n        all_emissions = []\n        for row in range(self.transitions.shape[0]):\n            all_selections.append(\n                rng.choice(\n                    a=len(self.initial),\n                    size=n_periods + burn_in_periods,\n                    p=self.transitions[row, :]\n                )\n            )\n            e = self.emissions[row][\"type\"].rvs(\n                **self.emissions[row][\"params\"], size=n_periods + burn_in_periods\n            )\n            if self.emissions[row][\"negative_emission\"]:\n                e = -e\n            all_emissions.append(e)\n\n        all_selections = np.vstack(all_selections).T\n        all_emissions = np.vstack(all_emissions).T\n\n        states = []\n        emissions = []\n        for row in range(n_periods + burn_in_periods):\n            if row == 0:\n                s = rng.choice(len(self.initial), p=self.initial)\n            else:\n                s = all_selections[row, s]\n\n            if row >= burn_in_periods:\n                states.append(s)\n                emissions.append(all_emissions[row, s])\n\n        return emissions, states\n\nhmm = HMM()\nhmm.fit(data)\ndf[\"hmm_states\"] = np.insert(hmm.states, 0, 0)\n```\n:::\n\n\nFirst, lets see how the model fit the states to the data:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nlabels = [\"panic\", \"normal\", \"relief\"]\nfig, ax1 = plt.subplots()\nax1.plot(df.index, df[\"s&p_comp_p\"], color=palette[0], label=\"s&p500\")\nax2 = ax1.twinx()\nax2.plot(df.index, df[\"hmm_states\"], color=\"#b0b0b0\", label=\"state\")\n\nax2.set_yticks([0, 1, 2])\nax2.set_yticklabels(labels)\n\nsns.despine(left=True, bottom=True, top=True, right=True)\nax1.set_yscale('log')\nformatter = ScalarFormatter()\nformatter.set_scientific(False)\nax1.yaxis.set_major_formatter(formatter)\n\nax1.set_zorder(1) # Plot ax1 on top of ax2\nax1.patch.set_visible(False) # Stop ax1 from hiding ax2\n# fig.legend(frameon=False, loc=(0.15,.8))\nax1.set_ylabel(\"s&p500_index\")\nax1.set_xlabel(\"date\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![S&P 500 Index with Estimated States](index_files/figure-html/fig-sp500_states-output-1.png){#fig-sp500_states width=657 height=429 fig-alt='Line chart showing S&P 500 performance since 1871, overlain by estimated states, Panic, Normal and Relief'}\n:::\n:::\n\n\nThis plot does indeed seem to pick up many of the notable market panics from the past hundred years, including the crash that started the great depression in the late 1920s, the tech bubble bursting in the early 2000s, the great financial crisis, etc. Notably, covid was not detected as the market fell and corrected within a month, so the crash isn't present in the month-end data.\n\nI called the positive distribution \"relief\", as it seems to only occur in periods following a panic. Let's see the transition probabilities:\n\n::: {#tbl-transition_probabilites .cell tbl-cap='S&P 500 State Transition Probabilities, showing probability of transitioning from state on vertical to the state on the horizontal.' execution_count=10}\n``` {.python .cell-code}\nfloat_format = [\".1%\" for _ in range(hmm.transitions.shape[1] + 1)] \npretty = tabulate(hmm.transitions, headers=labels, showindex = labels, tablefmt=\"html\", floatfmt=float_format)\ndisplay(HTML(pretty))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table>\n<thead>\n<tr><th>      </th><th style=\"text-align: right;\">  panic</th><th style=\"text-align: right;\">  normal</th><th style=\"text-align: right;\">  relief</th></tr>\n</thead>\n<tbody>\n<tr><td>panic </td><td style=\"text-align: right;\">  10.8%</td><td style=\"text-align: right;\">   83.8%</td><td style=\"text-align: right;\">    5.4%</td></tr>\n<tr><td>normal</td><td style=\"text-align: right;\">   1.0%</td><td style=\"text-align: right;\">   98.9%</td><td style=\"text-align: right;\">    0.1%</td></tr>\n<tr><td>relief</td><td style=\"text-align: right;\">   0.0%</td><td style=\"text-align: right;\">   67.4%</td><td style=\"text-align: right;\">   32.6%</td></tr>\n</tbody>\n</table>\n```\n:::\n:::\n\n\nThis tells us that the default state here is to stay in the \"business as usual\" normal state. From there, there is a 98.9% chance that the S&P 500 will stay in that state, following a Gaussian Distribution from one month to the next, with a 1.0% chance of moving into a panic state and a 0.1% chance of moving to a euphoric state. \n\nAs we noticed above, euphoria seems to be the wrong term for periods of unusually positive performance. It is more likely that we reach that state following a state of panic ($1.0\\%\\times 5.4\\% = 0.5\\%$) than following a normal state ($0.1\\%$). Perhaps it might be better termed \"relief\".\n\nFinally, let's look at the parameters for the emission probability distributions:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nprint(\nf\"panic:\\n  shape: {hmm.emissions[0]['params']['b']:0.2f}\\n  scale: -{hmm.emissions[0]['params']['scale']:0.2%}\\n\\\nnormal:\\n  mean: {hmm.emissions[1]['params']['loc']:0.2%}\\n  st.dev.: {hmm.emissions[1]['params']['scale']:0.2%}\\n\\\neuphoria:\\n  shape: {hmm.emissions[2]['params']['b']:0.2f}\\n  scale: {hmm.emissions[2]['params']['scale']:0.2%}\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\npanic:\n  shape: 1.65\n  scale: -5.03%\nnormal:\n  mean: 0.54%\n  st.dev.: 3.57%\neuphoria:\n  shape: 0.87\n  scale: 5.03%\n```\n:::\n:::\n\n\nAnd let's plot the emission probability distribution over histograms of returns for periods when the model estimates that the market was in each state:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\np = np.linalg.eig(hmm.transitions.T).eigenvectors[:, 0]\n# Normalize so probabilities sum to one.\nsteady_state_probabilities = p / np.sum(p)\nsteady_state_probabilities\n\ndef plot_states(state, ymax):\n    fig, ax1 = plt.subplots()\n    n_points = 200\n    xmin = -0.3\n    xmax=0.5\n    x = np.linspace(xmin, xmax, n_points)\n    dist = hmm.emissions[state]\n    y = dist[\"type\"].pdf(x, **dist[\"params\"]) #* steady_state_probabilities[state]\n    if dist[\"negative_emission\"]:\n        x = np.linspace(-xmin, -xmax, n_points)\n\n    ax1.plot(x, y, label = labels[state])\n\n    ax2 = ax1.twinx()\n\n    sns.histplot(df.loc[df[\"hmm_states\"]==state, \"s&p_perf\"], edgecolor=None, ax=ax2)\n    sns.despine(left=True, bottom=True, top=True, right=True)\n    plt.gca().xaxis.set_major_formatter(PercentFormatter(1))\n\n    sns.despine(left=True, bottom=True, top=True, right=True)\n    ax1.set_yticks([])\n    ax1.set_yticklabels([])\n    ax1.set_ylim([-0.1, None])\n\n    ax2.set_yticks([])\n    ax2.set_yticklabels([])\n    ax2.set_ylim([-0.01, ymax])\n\n    ax1.set_ylabel(\"probability_density\")\n    ax2.set_ylabel(\"count\")\n    ax1.set_xlabel(\"s&p500_monthly_performance\")\n    plt.show()\n\n\nplot_states(0, 20)\nplot_states(1, 150)\nplot_states(2, 20)\n```\n\n::: {.cell-output .cell-output-display}\n![Model Fit to Returns for Panic State](index_files/figure-html/fig-model_fit-output-1.png){#fig-model_fit width=577 height=429 fig-alt='Line chart showing S&P 500 performance since 1871, overlain by estimated states, Panic, Normal and Euphoria'}\n:::\n\n::: {.cell-output .cell-output-display}\n![Model Fit to Returns for Normal State](index_files/figure-html/fig-model_fit-output-2.png){#fig-model_fit width=577 height=429 fig-alt='Line chart showing S&P 500 performance since 1871, overlain by estimated states, Panic, Normal and Euphoria'}\n:::\n\n::: {.cell-output .cell-output-display}\n![Model Fit to Returns for Euphoria State](index_files/figure-html/fig-model_fit-output-3.png){#fig-model_fit width=577 height=429 fig-alt='Line chart showing S&P 500 performance since 1871, overlain by estimated states, Panic, Normal and Euphoria'}\n:::\n:::\n\n\nThese look like reasonable approximations, but more could probably be done to tighten them up.\n\n### Simulation\n\nOf course, the fun doesn't have to end here. We can use the model to create simulations about how the next 150 years might transpire. Here is a single rollout of 150 years:\n\n::: {#tbl-simulation .cell tbl-cap='Single simulated run of 150 years.' execution_count=13}\n``` {.python .cell-code}\nn_years = 150\nn_months = n_years * 12\nemissions, states = hmm.simulate(n_months, seed=19)\n\nx = np.linspace(1/12, n_years, n_months)\n\nlabels = [\"panic\", \"normal\", \"relief\"]\nfig, ax1 = plt.subplots()\n\nax1.plot(x, np.cumprod(np.array(emissions) + 1), color=palette[0], label=\"s&p500\")\nax2 = ax1.twinx()\nax2.plot(x, states, color=\"#b0b0b0\", label=\"state\")\n\nax2.set_yticks([0, 1, 2])\nax2.set_yticklabels(labels)\n\nsns.despine(left=True, bottom=True, top=True, right=True)\nax1.set_yscale('log')\nformatter = ScalarFormatter()\nformatter.set_scientific(False)\nax1.yaxis.set_major_formatter(formatter)\n\nax1.set_xticks(np.arange(min(x), max(x)+1, 25))\n\nax1.set_zorder(1) # Plot ax1 on top of ax2\nax1.patch.set_visible(False) # Stop ax1 from hiding ax2\n# fig.legend(frameon=False, loc=(0.15,.8))\nax1.set_ylabel(\"s&p500_index\")\nax1.set_xlabel(\"years\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/tbl-simulation-output-1.png){#tbl-simulation-1 width=657 height=429}\n:::\n:::\n\n\nThe above chart looks much like the last 150 years: generally up, with some \"lost decades\".\n\nAn interesting issue is that the model will sometimes generate a month of ruin, which looks like this:\n\n::: {#tbl-simulation-ruin .cell tbl-cap='Single simulated run of 150 years, showing ruin.' execution_count=14}\n``` {.python .cell-code}\nn_years = 150\nn_months = n_years * 12\nemissions, states = hmm.simulate(n_months, seed=18)\n\nx = np.linspace(1/12, n_years, n_months)\n\nlabels = [\"panic\", \"normal\", \"relief\"]\nfig, ax1 = plt.subplots()\n\nax1.plot(x, np.cumprod(np.array(emissions) + 1), color=palette[0], label=\"s&p500\")\nax2 = ax1.twinx()\nax2.plot(x, states, color=\"#b0b0b0\", label=\"state\")\n\nax2.set_yticks([0, 1, 2])\nax2.set_yticklabels(labels)\n\nsns.despine(left=True, bottom=True, top=True, right=True)\nax1.set_yscale('log')\nformatter = ScalarFormatter()\nformatter.set_scientific(False)\nax1.yaxis.set_major_formatter(formatter)\n\nax1.set_xticks(np.arange(min(x), max(x)+1, 25))\n\nax1.set_zorder(1) # Plot ax1 on top of ax2\nax1.patch.set_visible(False) # Stop ax1 from hiding ax2\n# fig.legend(frameon=False, loc=(0.15,.8))\nax1.set_ylabel(\"s&p500_index\")\nax1.set_xlabel(\"years\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/tbl-simulation-ruin-output-1.png){#tbl-simulation-ruin-1 width=649 height=429}\n:::\n:::\n\n\nOn the face of it, it looks like this might be an issue with the model, but actually this should be expected. We are looking at many periods of 150 years where wealth in the market is occasionally wiped out. For a few examples over the period where this actually happened in some of the most developed markets in the world, I found this interesting graphic:\n\n![](ruin.jpg){fig-align=\"center\"}\n\n## Summary\n\nAbove, we explained what a Hidden Markov Model (HMM) is with a simple example. Then we plunged into a much more interesting example: we showed that a HMM can be used to explain some of the biggest market incidents of the past 150 years, and can be used to simulate a future in which substantial and real negative tail event can be incorporated in a simulation.\n\n## Further Reading\n\nShiller, R. (2023), *Online Data - Robert Shiller*, Home Page of Robert Shiller, available at http://www.econ.yale.edu/~shiller/data.htm (accessed October 5, 2023).\n\nTaleb, N. N. (2020), *Statistical Consequences of Fat Tails: Real World Preasymptotics, Epistemology, and Applications: Papers and Commentary, The Technical Incerto Collection*, USA? STEM Academic Press.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}