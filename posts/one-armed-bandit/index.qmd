---
title: "Decision-Making - Beating the Bandits"
author: "Harlow Malloc"
date: "2023-08-03"
categories: []
image: "one-armed-bandits.png"
draft: true
toc: true
format:
    html:
        code-fold: true
---


```{python}
#  | include: false
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import seaborn as sns
from utilities.plot_template import the_template
```

![](man-machines.jpeg){fig-align="center"}

Decisions are hard. We can't see how things will turn out, and we never have all the information we need to make the best decision right now. However, there are systematic ways to work through the choices we make to give us a better chance of coming out on top.

One way to view things is through the lens of reinforcement learning. This is a framework for decision making that assumes there is some environment out there to act on, and whatever we decide to do the environment gives us some feedback in the form of a reward or a punishment. Through these interactions, we can learn a better decision making rule.

There is a little more to the framework, such as the idea of a "state" that can impact the results of our actions. But, we will ignore that for now for the classic model I will talk about below: the multi-armed bandit. This model has the following features:

- We are in a casino with slot machines (one-armed bandits[^1]).
- There are many machines to choose from.
- Expected reward can change over time.
- The reward on a given play is randomly scattered around the expected value.

    [^1]: The name comes from the fact that older mechanical slot machines had an arm on the side to make the machine work, and over the long run they will take all your money.

This type of decision can be seen in many real world situations, e.g. picking a restaurant, elements of a marketing campaign, or a vendor. This type of decision is also nice because it's relatively simple and it demonstrates some really nice elements of a decision-making strategy, which are the main takeaways from this blog:

> - **Exploration v. Exploitation**: a really key concept to grasp. Do you explore new restaurants every weekend like an epicurean nomad, or do you decide you've found one that's good enough (exploiting what you already know)? Spoiler: you try to balance both.
> - **Bias Towards Recency**: Things change all the time - you should change with them.
> - **Optimistic Initial Expectations**: Believe it or not, until you know any better you're better off assuming the best.

## Multi-Armed Bandits

Using a gambling machine allows us to bring this model squarely into the realm of probabilities, and it adds a natural system of rewards, i.e. prizes. So say, for example, you walk into a casino and start playing 3 machines. You go from one to the other over the course of 6 turns and get the following prizes. 

What do you do next?

```{python}
#| label: tbl-6_turns
#| tbl-cap: Results from 6 Turns

df = pd.DataFrame(
    {
        "turn": [1, 2, 3, 4, 5, 6],
        "machine_id": [1, 2, 3, 1, 2, 3],
        "prize": [0.10, 1.32, 0.29, 1.18, 1.10, 0.17]
    }
)

# Format table
d = dict(selector="th",
    props=[('text-align', 'center')])

(df.style.hide()
    .format({'prize': '${:,.2f}'})
    .set_properties(**{'width':'10em', 'text-align':'center'})
    .set_table_styles([d]))
```

I will be modelling the prizes won by a player as an exponential distribution. This seems appropriate because it will give us many small wins and a few very big wins. Below I have an example plot of the exponential distribution, with some sample rewards. As you can see, there are far more rewards below the mean than above it, but the big rewards are much bigger. The model will be set up with a cost to play each game of $1, and an expected reward of $0.95. The player will start with $100.

```{python}
# | label: fig-exponential_distribution_pdf
# | fig-cap: "Example of an Exponential Distribution"
# | fig-alt: "A plot of the exponential distribution probability density overlain by some sample points."

# Parameters
rng = np.random.default_rng(7)
mean = 0.99
lambda_param = 1.0 / mean
x = np.linspace(0, 6, 400)  # Generate x values
y = lambda_param * np.exp(-lambda_param * x)  # Exponential distribution function

# Sample 50 points and jitter for dodging
sample_points_x = rng.exponential(mean, 50)
jitter = 0.05  # Adjust this value for more/less jitter
sample_points_y = [rng.uniform(-jitter, jitter) for _ in range(50)]

palette = sns.color_palette()

sns.lineplot(x=x, y=y)
plt.plot([mean, mean], [lambda_param * np.exp(-lambda_param * mean), 0], color=palette[0])
plt.annotate("expected mean", (mean, 0.2), (1.5, 0.5),
    arrowprops=dict(facecolor=palette[0], edgecolor=palette[0],
        arrowstyle="simple,tail_width=0.07,head_width=0.7,head_length=1"))
plt.annotate("random samples", (2.5, 0), (4, 0.3),
    arrowprops=dict(facecolor=palette[0], edgecolor=palette[0],
        arrowstyle="simple,tail_width=0.07,head_width=0.7,head_length=1"))
sns.scatterplot(x=sample_points_x, y=sample_points_y)
# plt.grid(axis='y', color='grey', linestyle='--', linewidth=0.5)
sns.despine(left=True, bottom=True, top=True, right=True)
plt.tick_params(axis='x', which='both', bottom=True, left=True)

formatter = ticker.FuncFormatter(lambda x, pos: f'${x:.0f}')
plt.gca().xaxis.set_major_formatter(formatter)
plt.xlabel('prize')
plt.ylabel('probability density function')
plt.show()
```

Machines with an identical distribution of random prizes are not interesting though. Why? Well, there's no "good" machine to pick. Whether you pick one machine or hop around, you would have the same expected prize. The actual prize you win will vary through random chance which is not predictable.

Instead, we are interested in the cases where playing different machines has different expected outcomes. In this case, we want to have a systematic approach to deciding which machine to play to maximize our rewards. This maps back to other decisions we might make in our day-to-day lives, such as choosing a restaurant. The meals you get at a restaurant would probably have a less variability than my model here, but despite on-days and off-days, some restaurants are just better on average.

Creating a random walk with mean recursion for each of the three machines looks like this (noting that these are *expected* prizes, not actual prizes):

```{python}
# | label: fig-expected_reward
# | fig-cap: "Expected Rewards for each of 3 Machines"
# | fig-alt: "A line-plot of the expected rewards to each of 3 machines."

n_iterations = 500
n_machines = 3
n_turns = 1000

sigma = 0.2
cost_per_game = 1
expected_prize = 0.99
starting_value = 100

rng = np.random.default_rng(5)

mean_lin = np.ones(n_machines) * expected_prize
var_lin = np.ones(n_machines) * (sigma**2)

mean_log = np.log(mean_lin**2 / (np.sqrt(mean_lin**2 + var_lin)))
var_log = np.log(1 + var_lin / mean_lin**2) 
cov_log = np.diag(var_log)

# Mean-reverting process
noise_log = rng.multivariate_normal(np.zeros(n_machines), cov_log, size=(n_iterations, n_turns + 200))

e_rewards_log = np.ones_like(noise_log) * mean_log
theta = 0.01
for i in range(1, n_turns + 200):
    e_rewards_log[:, i, :] = e_rewards_log[:, i - 1, :] + (
        0.01 * (mean_log - e_rewards_log[:, i - 1, :]) + 0.15 * noise_log[:, i, :]
        )
e_rewards = np.exp(e_rewards_log[:, 200:])

# Generate rewards from expected rewards
rewards = rng.exponential(e_rewards)

machine_labels = [str(i + 1) for i in range(rewards.shape[2])]

rewards_df = pd.DataFrame(rewards[0, :, :])
rewards_df.columns = machine_labels
rewards_df["turn"] = list(range(1, rewards.shape[1] + 1))
rewards_df = rewards_df.melt(
    id_vars="turn",
    value_vars=machine_labels,
    value_name="prize",
    var_name="machine",)

e_rewards_df = pd.DataFrame(e_rewards[0, ::])
e_rewards_df.columns = machine_labels
e_rewards_df["turn"] = list(range(1, e_rewards.shape[1] + 1))
e_rewards_df = e_rewards_df.melt(
    id_vars="turn",
    value_vars=machine_labels,
    value_name="expected_prize",
    var_name="machine",)

rewards_df = rewards_df.merge(e_rewards_df, on=("turn", "machine"), how="left")

# Plot
ax = sns.lineplot(data=rewards_df, x="turn", y="expected_prize", hue="machine")
plt.grid(axis='y', color='#E5E5E5')
sns.despine(left=True, bottom=True, top=True, right=True)
plt.tick_params(axis='x', which='both', bottom=True, left=True)
plt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)
plt.ylim(0.6, 1.8)

formatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')
plt.gca().yaxis.set_major_formatter(formatter)
plt.show()
```

The actual prizes you would win if you played all the machines at the same time would look as follows:

```{python}
# | label: fig-reward
# | fig-cap: "Actual Rewards for each of 3 Machines"
# | fig-alt: "A scatter plot of the rewards to each of 3 machines."

ax = sns.scatterplot(data=rewards_df, x="turn", y="prize", hue="machine", s=7)
plt.grid(axis='y', color='#E5E5E5')
sns.despine(left=True, bottom=True, top=True, right=True)
plt.tick_params(axis='x', which='both', bottom=True, left=True)
plt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)
plt.ylim(0.0, 11)

formatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')
plt.gca().yaxis.set_major_formatter(formatter)
plt.show()
```

### The Oracle Strategy

We can also plot what we call the "oracle", which is a model with perfect information, i.e. it knows which machine is the best to play on each turn. The expected return to the oracle looks like this:

```{python}
# | label: fig-expected_reward_oracle
# | fig-cap: "Expected Prizes under the Oracle Strategy"
# | fig-alt: "A scatter plot of the prizes won by the Oracle Strategy."

oracle_e_rewards = np.max(e_rewards[0], axis=1)
turns = list(range(1, len(oracle_e_rewards)+1))

ax = sns.lineplot(data=rewards_df, x="turn", y="expected_prize", hue="machine", alpha=0.3)
sns.lineplot(x=turns, y=oracle_e_rewards, color=palette[4], label="oracle")

plt.grid(axis='y', color='#E5E5E5')
sns.despine(left=True, bottom=True, top=True, right=True)
plt.tick_params(axis='x', which='both', bottom=True, left=True)
plt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)
plt.tight_layout(rect=[0, 0, 1, 1])
plt.ylim(0.6, 1.8)

formatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')
plt.gca().yaxis.set_major_formatter(formatter)
plt.show()
```

We can track how successful the oracle strategy was over time by taking the running average of the prizes it won throughout the game:

That's not astonishing - even a strategy with perfect information is only barely coming out ahead - remember that each game costs $1 to play!

## The Random Strategy

Another strategy we could use is to pick a machine to play at random in each round. 

```{python}
# | label: fig-average_reward_random
# | fig-cap: "Running Average of Prizes won by Random Strategy "
# | fig-alt: "Running Average of Prizes won by Random Strategy."

# Oracle Strategy
oracle_choice = np.argmax(e_rewards, axis=2, keepdims=True)
oracle_rewards = np.take_along_axis(rewards, oracle_choice, axis=2)
oracle_rewards = np.squeeze(oracle_rewards)

turns = np.array(list(range(1, n_turns+1)))
average_oracle_rewards = np.cumsum(oracle_rewards, axis=1) / turns
average_oracle_rewards = np.mean(average_oracle_rewards, axis=0)

# Random Strategy
random_choice = rng.integers(0, n_machines, (n_iterations, n_turns))
random_choice = np.expand_dims(random_choice, 2)
random_rewards = np.take_along_axis(rewards, random_choice, axis=2)
random_rewards = np.squeeze(random_rewards)

turns = np.array(list(range(1, random_rewards.shape[1]+1)))
average_random_rewards = np.cumsum(random_rewards, axis=1) / turns
average_random_rewards = np.mean(average_random_rewards, axis=0)

# Plotting
sns.lineplot(x=turns, y=average_oracle_rewards, color=palette[4], label="oracle")
sns.lineplot(x=turns, y=average_random_rewards, color=palette[5], label="random")

plt.grid(axis='y', color='#E5E5E5')
sns.despine(left=True, bottom=True, top=True, right=True)
plt.tick_params(axis='x', which='both', bottom=True, left=True)
plt.legend(loc='upper left', bbox_to_anchor=(1, 1), frameon=False)
plt.tight_layout(rect=[0, 0, 0.98, 1])
plt.ylim(0.6, 1.8)
plt.xlabel("turn")
plt.ylabel("average_prize")

formatter = ticker.FuncFormatter(lambda x, pos: f'${x:.2f}')
plt.gca().yaxis.set_major_formatter(formatter)
plt.show()
```

## Exploration v. Exploitation

```{python}
def strategy(epsilon: float = 0.05, alpha: float=None, initial_value: float = None):
    # Initialize vectors
    running_value_estimate = np.zeros((n_iterations, n_turns, n_machines))
    running_count = np.zeros((n_iterations, n_turns, n_machines), dtype=int)
    exploit_rewards = np.zeros((n_iterations, n_turns))
    running_selection = np.zeros((n_iterations, n_turns), dtype=int)
    selection = np.zeros(n_iterations, dtype=int)[:, None]

    # Instantiate all random variables up front
    random_selection = rng.integers(low=0, high=n_machines, size=(n_iterations, n_turns))
    random_explore = rng.uniform(0, 1, size=(n_iterations, n_turns))
    for i in range(n_turns):
        if i < n_machines:
            # Try all machines once.
            selection = np.array([i]*n_iterations)[:, None]
        else:
            # Explore with some probability epsilon
            explore = random_explore[:, i] < epsilon
            selection[explore] = random_selection[explore, i][:, None]
            # Otherwise, use greedy selection (select machine thought most valuable)
            selection[~explore] = np.argmax(running_value_estimate[~explore, i-1, :], axis=1)[:, None]

        running_selection[:, i] = selection[:, 0]

        exploit_rewards[:, i] = np.take_along_axis(rewards[:, i, :], selection, axis=1)[:, 0]

        if i > 0:
            running_count[:, i, :] = running_count[:, i - 1, :]
        update_count = np.zeros((n_iterations, n_machines))
        np.put_along_axis(update_count, selection, 1, axis = 1)
        running_count[:, i, :] = running_count[:, i, :] + update_count

        if i < n_machines and initial_value is None:
            # If initial_value is None, start with initial value observed in machines.
            # NOTE: initial iterations could be randomized, but iterating along machines
            # 1, 2, 3, ... is random enough for this exercise.
            np.put_along_axis(running_value_estimate[:, i, :], selection, exploit_rewards[:, i][:, None], axis=1)
        else:
            if i == 0 and initial_value is not None:
                # If there is an initial_value, start with that.
                running_value_estimate[:, i, :] = initial_value
            else:
                running_value_estimate[:, i, :] = running_value_estimate[:, i - 1, :]

            if alpha is not None:
                # Exponential Weight Decay
                step_size = alpha
            else:
                # Incremental Mean Update
                step_size = 1/np.take_along_axis(running_count[:, i, :], selection, axis=1) 
            
            update_flat = (
                step_size
                * (
                    exploit_rewards[:, i][:, None] 
                    - np.take_along_axis(running_value_estimate[:, i, :], selection, axis=1))
            )
            update = np.zeros((n_iterations, n_machines))
            np.put_along_axis(update, selection, update_flat, axis=1)
            running_value_estimate[:, i, :] = running_value_estimate[:, i, :] + update

    return running_value_estimate, running_count, exploit_rewards, running_selection

strategy()
```


```{python}

```

For a more detailed exploration of the topic, I highly recommend the foundational Reinforcement Learning: An Introduction by Sutton and Barto.

