---
title: "Decision-Making - Beating the Bandits"
author: "Harlow Malloc"
date: "2023-08-03"
categories: []
image: "one-armed-bandits.png"
draft: true
toc: true
---


```{python}
#  | include: false
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import seaborn as sns
from utilities.plot_template import the_template
```

![](man-machines.jpeg){fig-align="center"}

Decisions are hard. We can't see how things will turn out, and we never have all the information we need to make the best decision right now. However, I believe there are systematic ways to approach many of the types of decisions we make in life.

One way to view things is through the lens of reinforcement learning. This is a framework for decision making that assumes there is some environment out there to act on, and whatever we decide to do, the environment gives us some feedback. We might get a reward or punishment, or end up in a new state (one we expected or not), and through these interactions, we try to come up with a better decision making rule.

There is a little more to the framework, such as the idea of a "state" that can impact the results of our actions. But, we will ignore that for now for the classic model I will talk about below: the multi-armed bandit. This model has the following features:

- We are in a casino with slot machines (one-armed bandits[^1]).
- There are many machines to choose from.
- Expected reward can change over time.
- The reward on a given play is randomly scattered around the expected value.

    [^1]: The name comes from the fact that older mechanical slot machines had an arm on the side to make the machine work, and over the long run they will take all your money.

This type of decision can be seen in many real world situations, e.g. picking a restaurant, elements of a marketing campaign, or a vendor. This type of decision is also nice because it's relatively simple and it demonstrates some really nice elements of a decision-making strategy, which are the main takeaways from this blog:

> - **Exploration v. Exploitation**: a really key concept to grasp. Do you explore new restaurants every weekend like an epicurean nomad, or do you decide you've found one that's good enough (exploiting what you already know)? Spoiler: you try to balance both.
> - **Optimistic Initial Expectations**: Believe it or not, until you know any better you're better off assuming the best.
> - **Bias Towards Recency**: Things change all the time - you should change with them.

## Multi-Armed Bandits

Using a gambling machine allows us to bring this model squarely into the realm of probabilities, and it adds a natural system of rewards. So say, for example, you walk into a casino and start playing 3 machines. You go from one to the other over the course of 6 turns and get the following prizes. 

What do you do next?
```{python}
#| echo: false

n_machines = 3
n_periods = 800

sigma = 0.2
cost_per_game = 1
expected_prize = 0.95
starting_value = 100

mean_lin = np.ones(n_machines) * expected_prize
var_lin = np.ones(n_machines) * (sigma**2)

mean_log = np.log(mean_lin**2 / (np.sqrt(mean_lin**2 + var_lin)))
var_log = np.log(1 + var_lin / mean_lin**2) 
cov_log = np.diag(var_log)
```

```{python}
# | echo: false
# Mean-reverting process
rng = np.random.default_rng(14)
noise_log = rng.multivariate_normal(np.zeros(n_machines), cov_log, size=(n_periods + 200))

e_rewards_log = np.ones_like(noise_log) * mean_log
theta = 0.01
for i in range(1, n_periods + 200):
    e_rewards_log[i, :] = e_rewards_log[i - 1, :] + (
        0.01 * (mean_log - e_rewards_log[i - 1, :]) + 0.15 * noise_log[i, :]
        )
e_rewards = np.exp(e_rewards_log[200:])

# Generate rewards from expected rewards
rewards = rng.exponential(e_rewards)

machine_labels = [str(i + 1) for i in range(rewards.shape[1])]

rewards_df = pd.DataFrame(rewards)
rewards_df.columns = machine_labels
rewards_df["turn"] = list(range(1, rewards.shape[0] + 1))
rewards_df = rewards_df.melt(
    id_vars="turn",
    value_vars=machine_labels,
    value_name="reward",
    var_name="machine",)

e_rewards_df = pd.DataFrame(e_rewards)
e_rewards_df.columns = machine_labels
e_rewards_df["turn"] = list(range(1, e_rewards.shape[0] + 1))
e_rewards_df = e_rewards_df.melt(
    id_vars="turn",
    value_vars=machine_labels,
    value_name="expected_reward",
    var_name="machine",)

rewards_df = rewards_df.merge(e_rewards_df, on=("turn", "machine"), how="left")
```

```{python}
#| echo: false
#| label: tbl-6_turns
#| tbl-cap: Results from 6 Turns
# Get the number of rows
num_rows = 6
# Randomly select an entry from each row
indices = [0, 1, 2, 0, 1, 2]
sample_rewards = rewards[np.arange(num_rows), indices]

df = pd.DataFrame(
    {
        "turn": list(range(1, num_rows + 1)),
        "machine_id": indices,
        "prize": sample_rewards,
    }
)

d = dict(selector="th",
    props=[('text-align', 'center')])

(df.style.hide()
    .format({'prize': '${:,.2f}'})
    .set_properties(**{'width':'10em', 'text-align':'center'})
    .set_table_styles([d]))

```

I will be modelling the returns to a player by playing the game as an exponential distribution. This seems appropriate because it will give us many small wins and a few very big wins. Below I have an example plot of the exponential distribution, with some sample sample points. As you can see, there are far more rewards below the mean than above it, but the big rewards are pretty big. The model will be set up with a cost to play each game of $1, and an expected reward of $0.95. The player will start with $100.

```{python}
# | label: fig-exponential_distribution_pdf
# | echo: false
# | fig-cap: "Exponential Distribution"
# | fig-alt: "A plot of the exponential distribution probability density overlain by some sample points."

# Parameters
mean = 0.95
lambda_param = 1.0 / mean
x = np.linspace(0, 6, 400)  # Generate x values
y = lambda_param * np.exp(-lambda_param * x)  # Exponential distribution function

# Sample 50 points and jitter for dodging
sample_points_x = rng.exponential(mean, 50)
jitter = 0.05  # Adjust this value for more/less jitter
sample_points_y = [rng.uniform(-jitter, jitter) for _ in range(50)]

palette = sns.color_palette()

sns.lineplot(x=x, y=y)
plt.plot([mean, mean], [lambda_param * np.exp(-lambda_param * mean), 0], color=palette[0])
plt.annotate("expected mean", (mean, 0.2), (1.5, 0.5),
    arrowprops=dict(facecolor=palette[0], edgecolor=palette[0],
        arrowstyle="simple,tail_width=0.07,head_width=0.7,head_length=1"))
plt.annotate("random samples", (2.5, 0), (4, 0.3),
    arrowprops=dict(facecolor=palette[0], edgecolor=palette[0],
        arrowstyle="simple,tail_width=0.07,head_width=0.7,head_length=1"))
sns.scatterplot(x=sample_points_x, y=sample_points_y)
# plt.grid(axis='y', color='grey', linestyle='--', linewidth=0.5)
sns.despine(left=True, bottom=True, top=True, right=True)
plt.tick_params(axis='x', which='both', bottom=True, left=True)

formatter = ticker.FuncFormatter(lambda x, pos: f'${x:.0f}')
plt.gca().xaxis.set_major_formatter(formatter)
plt.xlabel('prize')
plt.ylabel('probability density function')
plt.show()

```

Machines with an identical distribution of random rewards are not interesting though. Why? Well, there's no "good" machine to pick. Whether you pick one machine or hop around, you would have the same expected reward.

We are interested in the cases where playing different machines has different expected outcomes. In this case, we want to have a systematic approach to deciding which machine to play to maximize our rewards. This maps back to other decisions we might make in our day-to-day lives, such as choosing a restaurant. The meals you get at a restaurant would probably have a less variability than my model here, but despite on-days and off-days, some restaurants are just better on average.






```{python}
# | label: fig-expected_reward
# | echo: false
# | fig-cap: "Expected Rewards for each of 3 Machines"
# | fig-alt: "A line-plot of the expected rewards to each of 3 machines."

fig = px.line(
    rewards_df, x="turn", y="expected_reward", color="machine", template=the_template
)
fig.update_layout(
    xaxis_title="turn",
    yaxis_title="expected reward ($)",
    yaxis=dict(showgrid=True, ticklen=5, tickwidth=1),
    xaxis=dict(showgrid=False, ticklen=5, tickwidth=1),
)
fig.show()
```


```{python}
# | label: fig-reward
# | echo: false
# | fig-cap: "Actual Rewards for each of 3 Machines"
# | fig-alt: "A scatter plot of the rewards to each of 3 machines."

fig = px.scatter(
    rewards_df,
    x="turn",
    y="reward",
    color="machine",
    template=the_template,
)
# Layout
fig.update_layout(
    xaxis_title="turn",
    yaxis_title="reward ($)",
    yaxis=dict(showgrid=True, ticklen=5, tickwidth=1, ticks="outside"),
    xaxis=dict(
        showgrid=False, ticklen=5, tickwidth=1, zeroline=False, range=[0, n_periods]
    ),
)
fig.show()

#
#
```

For a more detailed exploration of the topic, I highly recommend the foundational Reinforcement Learning: An Introduction by Sutton and Barto.